{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-10T20:43:59.706765498Z",
     "start_time": "2023-10-10T20:43:59.659323118Z"
    }
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests as requests\n",
    "import time\n",
    "import numpy as np\n",
    "from scripts import user, item, discussion\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test chunck"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9c0978c01a736aa8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "titles = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for i in range(1,5):\n",
    "    \n",
    "    try:\n",
    "        url_posts = 'https://stacker.news/items/%d' % (i)\n",
    "        response = requests.get(url_posts)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "        title = soup.find_all('a', class_='item_title__FH7AS text-reset me-2')\n",
    "        \n",
    "        for e in title:\n",
    "            titles.append(e.get_text())\n",
    "        \n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "end = time.time()\n",
    "\n",
    "print(\"The time of execution of above program is :\",\n",
    "      (end-start), \"s\")\n",
    "print(titles)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c1b53dd5238f1a54"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Collect post header with all the related data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "22c54d832a42cfb6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "url_posts = 'https://stacker.news/items/279242'\n",
    "response = requests.get(url_posts)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "titolo = soup.find('a', class_='item_title__FH7AS text-reset me-2', href='/items/279242')\n",
    "\n",
    "\n",
    "banner = soup.find('div', class_='item_other__MjgP3')\n",
    "\n",
    "#post_stack = banner.find('span').text\n",
    "\n",
    "titolo_data = []\n",
    "\n",
    "# for i in titolo.find('span'):\n",
    "#     titolo_data.append(i.text)\n",
    "    \n",
    "#n_comments = soup.find('a', class_='text-reset position-relative').get_text()\n",
    "#nym_creator = banner.find('a', ) \n",
    "\n",
    "print(titolo==None)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "83f77a4a0af8c262"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Scraping Items of stacker.news\n",
    "\n",
    "### Scraping comments\n",
    "\n",
    "[Useful link to duplicate rows according to the values in a row. In our situation, we need to duplicate rows according to the usernames or according to the comment item number](https://saturncloud.io/blog/splitting-and-expanding-pandas-dataframes-based-on-column-values/)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b0413ab8459c1374"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "url_posts = 'https://stacker.news/items/278856'\n",
    "response = requests.get(url_posts)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "commenti_prova = []\n",
    "\n",
    "for i in soup.find_all('div', class_='item_item__Q_HbW comment_item__kLv_x'):\n",
    "    commenti_prova.append(i.get_text())\n",
    "    \n",
    "print(commenti_prova[0])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d39e7fd3732da90a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# This code collects the item number of every comment in a post\n",
    "\n",
    "import re\n",
    "url_posts = 'https://stacker.news/items/278856'\n",
    "response = requests.get(url_posts)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "commenters = []\n",
    "\n",
    "for i in tqdm(soup.find_all('a', class_='text-reset position-relative')):\n",
    "     c = i.get('href')\n",
    "     r = r'(\\D+)'\n",
    "     res = int(re.sub(r, '', c))\n",
    "     \n",
    "     commenters.append(res)\n",
    "      \n",
    "print(commenters)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a1fc059395aa864a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Testing of functions for Discussion extraction"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e5cf83fc6185ceba"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i in range(1,20):\n",
    "    url_posts = 'https://stacker.news/items/1'\n",
    "    response = requests.get(url_posts)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    discussion.scrape_discussion(279708, soup)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4d125614974cc780"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing the extraction of item type\n",
    "\n",
    "**Since job offers do not store a lot of data about interactions with users (very few comments, only a bunch of job offers into 300k items, etc) we could drop those and do not scrape them**\n",
    "In fact you can see them by simply looking at the home page of stacker.news and filter for 'job'"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bdb82e6d277cf4af"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 link\n",
      "2 discussion\n",
      "3 comment\n",
      "4 comment\n",
      "5 comment\n",
      "6 comment\n",
      "7 comment\n",
      "8 link\n",
      "9 comment\n",
      "10 comment\n",
      "11 comment\n",
      "12 comment\n",
      "13 comment\n",
      "14 comment\n",
      "15 comment\n",
      "16 comment\n",
      "17 comment\n",
      "18 comment\n",
      "19 comment\n",
      "20 comment\n",
      "21 link\n",
      "22 comment\n",
      "23 comment\n",
      "24 comment\n",
      "25 comment\n",
      "26 link\n",
      "27 comment\n",
      "28 comment\n",
      "29 comment\n",
      "30 comment\n",
      "31 link\n",
      "32 comment\n",
      "33 comment\n",
      "34 link\n",
      "35 link\n",
      "36 link\n",
      "37 comment\n",
      "38 link\n",
      "39 link\n",
      "40 link\n",
      "41 link\n",
      "42 link\n",
      "43 comment\n",
      "44 comment\n",
      "45 link\n",
      "46 comment\n",
      "47 link\n",
      "48 comment\n",
      "49 link\n",
      "50 comment\n",
      "51 link\n",
      "52 link\n",
      "53 link\n",
      "54 link\n",
      "55 comment\n",
      "56 link\n",
      "57 link\n",
      "58 link\n",
      "59 link\n",
      "60 link\n",
      "61 link\n",
      "62 link\n",
      "63 link\n",
      "64 comment\n",
      "65 comment\n",
      "66 link\n",
      "67 comment\n",
      "68 comment\n",
      "69 comment\n",
      "70 link\n",
      "71 comment\n",
      "72 link\n",
      "73 comment\n",
      "74 comment\n",
      "75 link\n",
      "76 link\n",
      "77 link\n",
      "78 link\n",
      "79 comment\n",
      "80 link\n",
      "81 link\n",
      "82 link\n",
      "83 link\n",
      "84 link\n",
      "85 link\n",
      "86 link\n",
      "87 comment\n",
      "88 link\n",
      "89 link\n",
      "90 link\n",
      "91 link\n",
      "92 link\n",
      "93 comment\n",
      "94 link\n",
      "95 comment\n",
      "96 comment\n",
      "97 comment\n",
      "98 comment\n",
      "99 link\n",
      "100 discussion\n",
      "101 comment\n",
      "102 comment\n",
      "103 comment\n",
      "104 comment\n",
      "105 comment\n",
      "106 comment\n",
      "107 comment\n",
      "108 comment\n",
      "109 comment\n",
      "110 comment\n",
      "111 comment\n",
      "112 comment\n",
      "113 comment\n",
      "114 comment\n",
      "115 comment\n",
      "116 comment\n",
      "117 comment\n",
      "118 discussion\n",
      "119 link\n",
      "120 link\n",
      "121 link\n",
      "122 link\n",
      "123 link\n",
      "124 link\n",
      "125 comment\n",
      "126 link\n",
      "127 link\n",
      "128 link\n",
      "129 comment\n",
      "130 link\n",
      "131 link\n",
      "132 comment\n",
      "133 comment\n",
      "134 comment\n",
      "135 comment\n",
      "136 comment\n",
      "137 comment\n",
      "138 comment\n",
      "139 comment\n",
      "140 link\n",
      "141 comment\n",
      "142 comment\n",
      "143 comment\n",
      "144 comment\n",
      "145 comment\n",
      "146 comment\n",
      "147 discussion\n",
      "148 comment\n",
      "149 comment\n",
      "150 comment\n",
      "151 comment\n",
      "152 comment\n",
      "153 comment\n",
      "154 comment\n",
      "155 comment\n",
      "156 link\n",
      "157 comment\n",
      "158 comment\n",
      "159 comment\n",
      "160 discussion\n",
      "161 link\n",
      "162 link\n",
      "163 link\n",
      "164 discussion\n",
      "165 comment\n",
      "166 link\n",
      "167 link\n",
      "168 link\n",
      "169 link\n",
      "170 link\n",
      "171 link\n",
      "172 discussion\n",
      "173 comment\n",
      "174 link\n",
      "175 comment\n",
      "176 link\n",
      "177 link\n",
      "178 link\n",
      "179 comment\n",
      "180 comment\n",
      "181 link\n",
      "182 comment\n",
      "183 link\n",
      "184 comment\n",
      "185 comment\n",
      "186 comment\n",
      "187 comment\n",
      "188 comment\n",
      "189 link\n",
      "190 comment\n",
      "191 comment\n",
      "192 comment\n",
      "193 comment\n",
      "194 link\n",
      "195 link\n",
      "196 comment\n",
      "197 comment\n",
      "198 link\n",
      "199 comment\n",
      "200 comment\n",
      "201 link\n",
      "202 comment\n",
      "203 comment\n",
      "204 comment\n",
      "205 comment\n",
      "206 comment\n",
      "207 comment\n",
      "208 link\n",
      "209 link\n",
      "210 link\n",
      "211 comment\n",
      "212 comment\n",
      "213 comment\n",
      "214 comment\n",
      "215 comment\n",
      "216 link\n",
      "217 comment\n",
      "218 comment\n",
      "219 comment\n",
      "220 link\n",
      "221 comment\n",
      "222 comment\n",
      "223 comment\n",
      "224 comment\n",
      "225 discussion\n",
      "226 link\n",
      "227 comment\n",
      "228 comment\n",
      "229 link\n",
      "230 link\n",
      "231 link\n",
      "232 comment\n",
      "233 link\n",
      "234 link\n",
      "235 link\n",
      "236 comment\n",
      "237 comment\n",
      "238 link\n",
      "239 link\n",
      "240 link\n",
      "241 comment\n",
      "242 comment\n",
      "243 comment\n",
      "244 comment\n",
      "245 comment\n",
      "246 link\n",
      "247 link\n",
      "248 link\n",
      "249 link\n",
      "250 link\n",
      "251 comment\n",
      "252 comment\n",
      "253 comment\n",
      "254 comment\n",
      "255 comment\n",
      "256 discussion\n",
      "257 comment\n",
      "258 comment\n",
      "259 comment\n",
      "260 comment\n",
      "261 comment\n",
      "262 comment\n",
      "263 comment\n",
      "264 comment\n",
      "265 comment\n",
      "266 comment\n",
      "267 comment\n",
      "268 comment\n",
      "269 comment\n",
      "270 comment\n",
      "271 comment\n",
      "272 link\n",
      "273 comment\n",
      "274 link\n",
      "275 link\n",
      "276 comment\n",
      "277 comment\n",
      "278 comment\n",
      "279 link\n",
      "280 link\n",
      "281 comment\n",
      "282 comment\n",
      "283 comment\n",
      "284 comment\n",
      "285 link\n",
      "286 comment\n",
      "287 link\n",
      "288 comment\n",
      "289 comment\n",
      "290 comment\n",
      "291 link\n",
      "292 comment\n",
      "293 link\n",
      "294 link\n",
      "295 comment\n",
      "296 link\n",
      "297 comment\n",
      "298 link\n",
      "299 comment\n"
     ]
    }
   ],
   "source": [
    "# for n in [277155, 127070, 277394, 278874, 277840, 239180, 235708, 260050]:\n",
    "#     url_posts = f'https://stacker.news/items/{n}'\n",
    "#     response = requests.get(url_posts)\n",
    "#     soup = BeautifulSoup(response.text, 'html.parser')\n",
    "#     print(n, item.detect_item_type(n, soup))\n",
    "    \n",
    "for n in range(1,300):\n",
    "    url_posts = f'https://stacker.news/items/{n}'\n",
    "    response = requests.get(url_posts)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    print(n, item.detect_item_type(n, soup))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-10T20:52:31.094177370Z",
     "start_time": "2023-10-10T20:49:27.033499557Z"
    }
   },
   "id": "55c3129f32c5d366"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # Function to recursively scrape comments and capture hierarchy\n",
    "# def scrape_comments(comment_element, depth=0):\n",
    "#     comment_text = comment_element.find('div', class_='comment-text').get_text().strip()\n",
    "#     \n",
    "#     # Print the comment with proper indentation based on depth\n",
    "#     print('  ' * depth + comment_text)\n",
    "#     \n",
    "#     # Recursively scrape child comments\n",
    "#     child_comments = comment_element.find_all('div', class_='comment')\n",
    "#     for child_comment in child_comments:\n",
    "#         scrape_comments(child_comment, depth + 1)\n",
    "# \n",
    "# def scrape_post_comments(post_url):\n",
    "#     response = requests.get(post_url)\n",
    "#     if response.status_code != 200:\n",
    "#         print(\"Failed to fetch the page\")\n",
    "#         return\n",
    "# \n",
    "#     soup = BeautifulSoup(response.text, 'html.parser')\n",
    "#     \n",
    "#     # Find and scrape the post content\n",
    "#     post_content = soup.find('div', class_='post-content').get_text().strip()\n",
    "#     print(\"Post Content:\")\n",
    "#     print(post_content)\n",
    "#     \n",
    "#     # Find and scrape top-level comments\n",
    "#     top_level_comments = soup.find_all('div', class_='comment-root')\n",
    "#     \n",
    "#     print(\"\\nComments:\")\n",
    "#     for comment in top_level_comments:\n",
    "#         scrape_comments(comment)\n",
    "# \n",
    "# if __name__ == \"__main__\":\n",
    "#     post_url = \"https://example.com/post-url\"  # Replace with the URL of the post you want to scrape\n",
    "#     scrape_post_comments(post_url)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9cbb32db4d3c06ca"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Reframing the previous code\n",
    "Main steps in the code\n",
    "1. Retrieve item webpage provided the item code\n",
    "2. Detect item type:\n",
    "    - Comment or post ?\n",
    "    - If post, which kind of post:\n",
    "        1. Discussion\n",
    "        2. Link\n",
    "        3. Poll\n",
    "        4. Bounty\n",
    "        5. Job\n",
    "3. Retrieve title\n",
    "4. Retrieve banner\n",
    "    - Extract number of comment, **compulsory**\n",
    "    - Extract stacked amount by the item, **if present**\n",
    "    - Extract Boost value, **if present**\n",
    "    - Extract username, **compulsory**\n",
    "    - Extract timestamp, **compulsory**\n",
    "    - Extract badge, **compulsory**\n",
    "5. Extract amount stacked by comments, **compulsory**\n",
    "6. Extract item code of comments **OR** extract user that commented\n",
    "\n",
    "**Note that**:\n",
    "- Some items do not have the stacked amount nor the possibility to receive sats. For example the user @saloon created all this kind of posts. Is he/she a bot? Is it an 'official bot' of the forum and so it's not possible to give sats to it?\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7cd9ceda58f8bbc2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Scraping user profiles\n",
    "Users profiles are scraped starting from the list of users extracted by scraping all the items (posts+comments)\n",
    "The link to get the user profile is `https://stacker.news/$username$` "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7771202dfaced9e6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "user_list = ['Monotone',\n",
    "             'TNStacker',\n",
    "             'kale',\n",
    "             'DiracDelta',\n",
    "             'kr',\n",
    "             'moscowTimeBot',\n",
    "             'mpuels',\n",
    "             'blockstream_official',\n",
    "             'nym',\n",
    "             0,\n",
    "             'random_',\n",
    "             'saloon',\n",
    "             \"k00b\",\n",
    "             \"utente che non esiste per niente\",\n",
    "             \"DarthCoin\",\n",
    "             \"Wumbo\",\n",
    "             \"mf\",\n",
    "             \"NoStranger\",\n",
    "             \"anipy\",\n",
    "             \"OneOneSeven\",\n",
    "             \"Bitman\",\n",
    "             \"nemo\",\n",
    "             \"sahil\",\n",
    "             \"prova_di_nullo\",\n",
    "             \"babababa nullo\",\n",
    "             None,\n",
    "             ]\n",
    "\n",
    "user_list2 = [\"k00b\", \"DarthCoin\", \"saloon\"]\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "user.save_profile_csv(user_list)\n",
    "\n",
    "end = time.time()\n",
    "print(\"The provided entries are \", len(user_list),\"\\nThe average time of execution of above program for every entry is :\",\n",
    "      (end-start)/len(user_list), \"\\nThe total time of execution is \", (end-start))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "872713e01a844e3a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Modularize the code - user profile scraping\n",
    "Define functions to modularize and simplify the user profile scraping.\n",
    "The functions are defined in `user_modules/scraping_user.py` and have been tested with `tests/test_scraping_user.py`. To run the tests open a the terminal/CMD and run the script `tests/test_scraping_user.py` from there, running it from a JupyterNotebook could raise errors.\n",
    "The testing script tested every function in some corner cases (missing data/request error). "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6e52500e0ab6223e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**From the following two approaches we can create the final script for scraping the user profiles**.\n",
    "The next step would be the creation of a function that loops through the user list and assigns to the rows in the dataframe the values returned from the functions defined in `user_modules/scraping_user.py`."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "efe3fde8a3810bc4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: integrate this method into the general scraping script for profile scraping\n",
    "\n",
    "user_list = np.array(['Monotone',\n",
    "                      'TNStacker',\n",
    "                      'kale',\n",
    "                      'DiracDelta',\n",
    "                      'kr',\n",
    "                      'moscowTimeBot',\n",
    "                      'mpuels',\n",
    "                      'blockstream_official',\n",
    "                      'nym',\n",
    "                      'random_'\n",
    "                      ])\n",
    "                      \n",
    "start = time.time()\n",
    "\n",
    "for i in np.nditer(user_list):\n",
    "    print(user.get_profile(i))\n",
    "    \n",
    "end = time.time()\n",
    "print(\"The average time of execution of the above program for one user is :\",\n",
    "      (end-start)/len(user_list), \"s\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "19462e10c12c6aed"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "user_list = ['Monotone',\n",
    "             'TNStacker',\n",
    "             'kale',\n",
    "             'DiracDelta',\n",
    "             'kr',\n",
    "             'moscowTimeBot',\n",
    "             'mpuels',\n",
    "             'blockstream_official',\n",
    "             'nym',\n",
    "             'random_',\n",
    "             ]\n",
    "                      \n",
    "start = time.time()\n",
    "\n",
    "for i in user_list:\n",
    "    print(user.get_profile(i))\n",
    "    \n",
    "end = time.time()\n",
    "print(\"The average time of execution of the above program for one user is :\",\n",
    "      (end-start)/len(user_list), \"s\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "930f937ca7b5188f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
