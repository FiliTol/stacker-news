{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-10T06:45:31.416599324Z",
     "start_time": "2023-10-10T06:45:30.689185322Z"
    }
   },
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "from bs4 import BeautifulSoup\n",
    "import requests as requests\n",
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scripts import user, item\n",
    "#from user_modules import scraping_item as si   # to be substituted with scripts.item\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test chunck"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9c0978c01a736aa8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "titles = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for i in range(1,5):\n",
    "    \n",
    "    try:\n",
    "        url_posts = 'https://stacker.news/items/%d' % (i)\n",
    "        response = requests.get(url_posts)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "        title = soup.find_all('a', class_='item_title__FH7AS text-reset me-2')\n",
    "        \n",
    "        for e in title:\n",
    "            titles.append(e.get_text())\n",
    "        \n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "end = time.time()\n",
    "\n",
    "print(\"The time of execution of above program is :\",\n",
    "      (end-start), \"s\")\n",
    "print(titles)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c1b53dd5238f1a54"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Collect post header with all the related data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "22c54d832a42cfb6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "url_posts = 'https://stacker.news/items/279242'\n",
    "response = requests.get(url_posts)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "titolo = soup.find('a', class_='item_title__FH7AS text-reset me-2', href='/items/279242')\n",
    "\n",
    "\n",
    "banner = soup.find('div', class_='item_other__MjgP3')\n",
    "\n",
    "#post_stack = banner.find('span').text\n",
    "\n",
    "titolo_data = []\n",
    "\n",
    "# for i in titolo.find('span'):\n",
    "#     titolo_data.append(i.text)\n",
    "    \n",
    "#n_comments = soup.find('a', class_='text-reset position-relative').get_text()\n",
    "#nym_creator = banner.find('a', ) \n",
    "\n",
    "print(titolo==None)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "83f77a4a0af8c262"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Scraping Items of stacker.news"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b0413ab8459c1374"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a list of the items to scrape\n",
    "n_posts = range(1, 10)\n",
    "\n",
    "df = pd.DataFrame(columns=['item','title', 'n_comments','corpus' ,'boost', 'sats', 'betha','commentors','external_links'])\n",
    "\n",
    "for number in n_posts:\n",
    "    \n",
    "    # Initialize the dataframe\n",
    "    url_posts = f'https://stacker.news/items/{number}'\n",
    "    response = requests.get(url_posts)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    index = number-1\n",
    "#-------------------------------------------------------------------------------\n",
    "    # Title\n",
    "    try:\n",
    "        title = soup.find('a', class_='item_title__FH7AS text-reset me-2').get_text()\n",
    "    except:\n",
    "        continue\n",
    "    # add title to dataframe\n",
    "    df.at[index, 'title'] = title\n",
    "#-------------------------------------------------------------------------------\n",
    "    # ITEM\n",
    "    df.at[index, 'item'] = number\n",
    "#-------------------------------------------------------------------------------\n",
    "    # CORPUS\n",
    "    try:\n",
    "        corpus=soup.find('div', class_=\"item_fullItemContainer__ZAYtZ\").get_text()\n",
    "    except:\n",
    "        df.at[index, 'corpus'] = \"NaN\"\n",
    "    # add title to dataframe\n",
    "    df.at[index, 'corpus'] = corpus\n",
    "#-------------------------------------------------------------------------------\n",
    "    # EXTERNAL LINKS\n",
    "    try:\n",
    "        ex_link=soup.find('a', class_=\"item_link__4cWVs\").get_text()\n",
    "    except:\n",
    "        df.at[index, 'external_links'] = \"NaN\"\n",
    "    # add external links to dataframe\n",
    "    df.at[index, 'external_links'] = ex_link\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "    # BANNER\n",
    "    try:\n",
    "        banner = soup.find('div', class_='item_other__MjgP3')\n",
    "    except:\n",
    "        df.at[index, 'boost'] = \"NaN\"\n",
    "        df.at[index, 'sats'] = \"NaN\"\n",
    "        df.at[index, 'betha'] = \"NaN\"\n",
    "    # deal with banner\n",
    "    banner_data = []\n",
    "    for i in banner.find_all('span'):\n",
    "        banner_data.append(i.text)\n",
    "    for b in banner_data:\n",
    "        if \"boost\" in b:\n",
    "            df.at[index, 'boost'] = b\n",
    "        if \"sats\" in b:\n",
    "            df.at[index, 'sats'] = b\n",
    "        if \"@\" in b:\n",
    "            df.at[index, 'betha'] = b\n",
    "#-------------------------------------------------------------------------------\n",
    "    # N_COMMENTS\n",
    "    try:\n",
    "        n_comments = soup.find('a', class_='text-reset position-relative').get_text()\n",
    "    except:        \n",
    "        df.at[index, 'n_comments'] = \"NaN\"\n",
    "    # add n_comments to dataframe\n",
    "    df.at[index, 'n_comments'] = n_comments\n",
    "#-------------------------------------------------------------------------------\n",
    "    # COMMENTORS\n",
    "    \n",
    "    a_elements = soup.find_all('a')\n",
    "    \n",
    "    df.at[index, 'commentors'] = \"NaN\"\n",
    "    at_elements = []\n",
    "\n",
    "    for el in a_elements:\n",
    "        links = el.get_text() \n",
    "        if links.startswith('@'):\n",
    "            at_elements.append(el)\n",
    "    \n",
    "    commentors_list=[]\n",
    "    for ind in range(0,len(at_elements)):\n",
    "        commentors_list.append(at_elements[ind][\"href\"])\n",
    "    # add commentors to dataframe\n",
    "    df.at[index, 'commentors'] = commentors_list\n",
    "#-------------------------------------------------------------------------------\n",
    "# Fixing the dataframe\n",
    "df['author'] = df['betha'].str.extract(r'@(\\w+)')\n",
    "df['data'] = df['betha'].str.extract(r'(\\d+\\s\\w+\\s\\d+)')\n",
    "df.drop('betha', axis=1, inplace=True)\n",
    "df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dd5f21f2f7aff4e5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "url_posts = 'https://stacker.news/items/278856'\n",
    "response = requests.get(url_posts)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "commenti_prova = []\n",
    "\n",
    "for i in soup.find_all('div', class_='item_item__Q_HbW comment_item__kLv_x'):\n",
    "    commenti_prova.append(i.get_text())\n",
    "    \n",
    "print(commenti_prova[0])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d39e7fd3732da90a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# This code collects the item number of every comment in a post\n",
    "\n",
    "import re\n",
    "url_posts = 'https://stacker.news/items/278856'\n",
    "response = requests.get(url_posts)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "commenters = []\n",
    "\n",
    "for i in soup.find_all('a', class_='text-reset position-relative'):\n",
    "     c = i.get('href')\n",
    "     r = r'(\\D+)'\n",
    "     res = int(re.sub(r, '', c))\n",
    "     \n",
    "     commenters.append(res)\n",
    "      \n",
    "print(commenters)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a1fc059395aa864a"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 link\n",
      "2 post\n",
      "3 comment\n",
      "4 comment\n",
      "5 comment\n",
      "6 comment\n",
      "7 comment\n",
      "8 link\n",
      "9 comment\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,10):\n",
    "    \n",
    "    url_posts = f'https://stacker.news/items/{i}'\n",
    "    response = requests.get(url_posts)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    category = 'comment'\n",
    "    \n",
    "    if item.detect_title(i, soup):\n",
    "        category = 'post'\n",
    "        \n",
    "        if item.detect_item_link(soup):\n",
    "            category = 'link'\n",
    "        elif item.detect_item_bounty(soup):\n",
    "            category = 'bounty'\n",
    "        elif item.detect_item_poll(soup):\n",
    "            category = 'poll'\n",
    "        elif item.detect_item_job(soup):\n",
    "            category = 'job'\n",
    "        \n",
    "    print(i, category)\n",
    "    \n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-10T06:50:47.257882643Z",
     "start_time": "2023-10-10T06:50:37.763152987Z"
    }
   },
   "id": "576177f0354260f9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**NB**\n",
    "- job offers have the title\n",
    "- links have a title\n",
    "- bounties have title\n",
    "- polls have title\n",
    "\n",
    "**Add the DETECT_DISCUSSION function in order to classify the different kind of posts and do not leave the general 'post' lable**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fa73b4e54d000a75"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "'POLL: Would you become immortal, if you could? '"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_posts = f'https://stacker.news/items/233918'\n",
    "response = requests.get(url_posts)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "item.detect_title(233918, soup)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-10T06:49:20.520106577Z",
     "start_time": "2023-10-10T06:49:18.515795682Z"
    }
   },
   "id": "55c3129f32c5d366"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # Function to recursively scrape comments and capture hierarchy\n",
    "# def scrape_comments(comment_element, depth=0):\n",
    "#     comment_text = comment_element.find('div', class_='comment-text').get_text().strip()\n",
    "#     \n",
    "#     # Print the comment with proper indentation based on depth\n",
    "#     print('  ' * depth + comment_text)\n",
    "#     \n",
    "#     # Recursively scrape child comments\n",
    "#     child_comments = comment_element.find_all('div', class_='comment')\n",
    "#     for child_comment in child_comments:\n",
    "#         scrape_comments(child_comment, depth + 1)\n",
    "# \n",
    "# def scrape_post_comments(post_url):\n",
    "#     response = requests.get(post_url)\n",
    "#     if response.status_code != 200:\n",
    "#         print(\"Failed to fetch the page\")\n",
    "#         return\n",
    "# \n",
    "#     soup = BeautifulSoup(response.text, 'html.parser')\n",
    "#     \n",
    "#     # Find and scrape the post content\n",
    "#     post_content = soup.find('div', class_='post-content').get_text().strip()\n",
    "#     print(\"Post Content:\")\n",
    "#     print(post_content)\n",
    "#     \n",
    "#     # Find and scrape top-level comments\n",
    "#     top_level_comments = soup.find_all('div', class_='comment-root')\n",
    "#     \n",
    "#     print(\"\\nComments:\")\n",
    "#     for comment in top_level_comments:\n",
    "#         scrape_comments(comment)\n",
    "# \n",
    "# if __name__ == \"__main__\":\n",
    "#     post_url = \"https://example.com/post-url\"  # Replace with the URL of the post you want to scrape\n",
    "#     scrape_post_comments(post_url)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9cbb32db4d3c06ca"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Reframing the previous code\n",
    "Main steps in the code\n",
    "1. Retrieve item webpage provided the item code\n",
    "2. Detect item type:\n",
    "    - Comment or post ?\n",
    "    - If post, which kind of post:\n",
    "        1. Discussion\n",
    "        2. Link\n",
    "        3. Poll\n",
    "        4. Bounty\n",
    "        5. Job\n",
    "3. Retrieve title\n",
    "4. Retrieve banner\n",
    "    - Extract number of comment, **compulsory**\n",
    "    - Extract stacked amount by the item, **if present**\n",
    "    - Extract Boost value, **if present**\n",
    "    - Extract username, **compulsory**\n",
    "    - Extract timestamp, **compulsory**\n",
    "    - Extract badge, **compulsory**\n",
    "5. Extract amount stacked by comments, **compulsory**\n",
    "6. Extract item code of comments **OR** extract user that commented\n",
    "\n",
    "**Note that**:\n",
    "- Some items do not have the stacked amount nor the possibility to receive sats. For example the user @saloon created all this kind of posts. Is he/she a bot? Is it an 'official bot' of the forum and so it's not possible to give sats to it?\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7cd9ceda58f8bbc2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Detecting item types\n",
    "First of all we need to determine the kind of item we are working with.\n",
    "The typology is described by the tag of the post in the post-creation."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e7397c7e162d9e3d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "reload(si)\n",
    "\n",
    "NA = None\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d0864f7fbcca9cdf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Scraping user profiles\n",
    "Users profiles are scraped starting from the list of users extracted by scraping all the items (posts+comments)\n",
    "The link to get the user profile is `https://stacker.news/$username$` "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7771202dfaced9e6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "user_list = ['Monotone',\n",
    "             'TNStacker',\n",
    "             'kale',\n",
    "             'DiracDelta',\n",
    "             'kr',\n",
    "             'moscowTimeBot',\n",
    "             'mpuels',\n",
    "             'blockstream_official',\n",
    "             'nym',\n",
    "             0,\n",
    "             'random_',\n",
    "             'saloon',\n",
    "             \"k00b\",\n",
    "             \"utente che non esiste per niente\",\n",
    "             \"DarthCoin\",\n",
    "             \"Wumbo\",\n",
    "             \"mf\",\n",
    "             \"NoStranger\",\n",
    "             \"anipy\",\n",
    "             \"OneOneSeven\",\n",
    "             \"Bitman\",\n",
    "             \"nemo\",\n",
    "             \"sahil\",\n",
    "             \"prova_di_nullo\",\n",
    "             \"babababa nullo\",\n",
    "             None,\n",
    "             ]\n",
    "\n",
    "user_list2 = [\"k00b\", \"DarthCoin\", \"saloon\"]\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "user.save_profile_csv(user_list)\n",
    "\n",
    "end = time.time()\n",
    "print(\"The provided entries are \", len(user_list),\"\\nThe average time of execution of above program for every entry is :\",\n",
    "      (end-start)/len(user_list), \"\\nThe total time of execution is \", (end-start))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "872713e01a844e3a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Modularize the code - user profile scraping\n",
    "Define functions to modularize and simplify the user profile scraping.\n",
    "The functions are defined in `user_modules/scraping_user.py` and have been tested with `tests/test_scraping_user.py`. To run the tests open a the terminal/CMD and run the script `tests/test_scraping_user.py` from there, running it from a JupyterNotebook could raise errors.\n",
    "The testing script tested every function in some corner cases (missing data/request error). "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6e52500e0ab6223e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**From the following two approaches we can create the final script for scraping the user profiles**.\n",
    "The next step would be the creation of a function that loops through the user list and assigns to the rows in the dataframe the values returned from the functions defined in `user_modules/scraping_user.py`."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "efe3fde8a3810bc4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: integrate this method into the general scraping script for profile scraping\n",
    "\n",
    "user_list = np.array(['Monotone',\n",
    "                      'TNStacker',\n",
    "                      'kale',\n",
    "                      'DiracDelta',\n",
    "                      'kr',\n",
    "                      'moscowTimeBot',\n",
    "                      'mpuels',\n",
    "                      'blockstream_official',\n",
    "                      'nym',\n",
    "                      'random_'\n",
    "                      ])\n",
    "                      \n",
    "start = time.time()\n",
    "\n",
    "for i in np.nditer(user_list):\n",
    "    print(user.get_profile(i))\n",
    "    \n",
    "end = time.time()\n",
    "print(\"The average time of execution of the above program for one user is :\",\n",
    "      (end-start)/len(user_list), \"s\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "19462e10c12c6aed"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "user_list = ['Monotone',\n",
    "             'TNStacker',\n",
    "             'kale',\n",
    "             'DiracDelta',\n",
    "             'kr',\n",
    "             'moscowTimeBot',\n",
    "             'mpuels',\n",
    "             'blockstream_official',\n",
    "             'nym',\n",
    "             'random_',\n",
    "             ]\n",
    "                      \n",
    "start = time.time()\n",
    "\n",
    "for i in user_list:\n",
    "    print(user.get_profile(i))\n",
    "    \n",
    "end = time.time()\n",
    "print(\"The average time of execution of the above program for one user is :\",\n",
    "      (end-start)/len(user_list), \"s\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "930f937ca7b5188f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
