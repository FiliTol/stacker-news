{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-10T13:09:59.608309878Z",
     "start_time": "2023-10-10T13:09:59.564701763Z"
    }
   },
   "outputs": [],
   "source": [
    "#from importlib import reload\n",
    "from bs4 import BeautifulSoup\n",
    "import requests as requests\n",
    "import time\n",
    "#import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scripts import user, item, discussion\n",
    "#import csv\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test chunck"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9c0978c01a736aa8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "titles = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for i in range(1,5):\n",
    "    \n",
    "    try:\n",
    "        url_posts = 'https://stacker.news/items/%d' % (i)\n",
    "        response = requests.get(url_posts)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "        title = soup.find_all('a', class_='item_title__FH7AS text-reset me-2')\n",
    "        \n",
    "        for e in title:\n",
    "            titles.append(e.get_text())\n",
    "        \n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "end = time.time()\n",
    "\n",
    "print(\"The time of execution of above program is :\",\n",
    "      (end-start), \"s\")\n",
    "print(titles)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c1b53dd5238f1a54"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Collect post header with all the related data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "22c54d832a42cfb6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "url_posts = 'https://stacker.news/items/279242'\n",
    "response = requests.get(url_posts)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "titolo = soup.find('a', class_='item_title__FH7AS text-reset me-2', href='/items/279242')\n",
    "\n",
    "\n",
    "banner = soup.find('div', class_='item_other__MjgP3')\n",
    "\n",
    "#post_stack = banner.find('span').text\n",
    "\n",
    "titolo_data = []\n",
    "\n",
    "# for i in titolo.find('span'):\n",
    "#     titolo_data.append(i.text)\n",
    "    \n",
    "#n_comments = soup.find('a', class_='text-reset position-relative').get_text()\n",
    "#nym_creator = banner.find('a', ) \n",
    "\n",
    "print(titolo==None)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "83f77a4a0af8c262"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Scraping Items of stacker.news\n",
    "\n",
    "### Scraping comments\n",
    "\n",
    "[Useful link to duplicate rows according to the values in a row. In our situation, we need to duplicate rows according to the usernames or according to the comment item number](https://saturncloud.io/blog/splitting-and-expanding-pandas-dataframes-based-on-column-values/)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b0413ab8459c1374"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110 sats \\ 4 replies \\ @siggy47  18hFascinating article. I guess we all can relate to having experienced audience capture. I'm sure I don't even realize how it's affected me and to what extent, but I have noticed it on occasion. It's usually when I write a thoughtless, automatic post or reply, relying on groupthink rather than my own beliefs.\n",
      "Thanks for posting.\n"
     ]
    }
   ],
   "source": [
    "url_posts = 'https://stacker.news/items/278856'\n",
    "response = requests.get(url_posts)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "commenti_prova = []\n",
    "\n",
    "for i in soup.find_all('div', class_='item_item__Q_HbW comment_item__kLv_x'):\n",
    "    commenti_prova.append(i.get_text())\n",
    "    \n",
    "print(commenti_prova[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-10T08:04:38.102979398Z",
     "start_time": "2023-10-10T08:04:36.618014884Z"
    }
   },
   "id": "d39e7fd3732da90a"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 47051.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[278856, 278922, 278948, 279342, 279367, 278961, 279563]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# This code collects the item number of every comment in a post\n",
    "\n",
    "import re\n",
    "url_posts = 'https://stacker.news/items/278856'\n",
    "response = requests.get(url_posts)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "commenters = []\n",
    "\n",
    "for i in tqdm(soup.find_all('a', class_='text-reset position-relative')):\n",
    "     c = i.get('href')\n",
    "     r = r'(\\D+)'\n",
    "     res = int(re.sub(r, '', c))\n",
    "     \n",
    "     commenters.append(res)\n",
    "      \n",
    "print(commenters)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-10T12:34:45.573439333Z",
     "start_time": "2023-10-10T12:34:43.419260699Z"
    }
   },
   "id": "a1fc059395aa864a"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "['553 sats', ' \\\\ ', ' \\\\ ', '@elvismercury  9 Oct', ' ', '', ' ', 'meta', '']"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_posts = 'https://stacker.news/items/278856'\n",
    "response = requests.get(url_posts)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "#discussion.extract_title(soup)\n",
    "discussion.extract_banner(soup)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-10T13:14:49.859786725Z",
     "start_time": "2023-10-10T13:14:48.714500116Z"
    }
   },
   "id": "4d125614974cc780"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "40e654d8e0ae41f0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing the extraction of item type\n",
    "\n",
    "**Since job offers do not store a lot of data about interactions with users (very few comments, only a bunch of job offers into 300k items, etc) we could drop those and do not scrape them**\n",
    "In fact you can see them by simply looking at the home page of stacker.news and filter for 'job'"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bdb82e6d277cf4af"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "277155 discussion\n",
      "127070 bounty\n",
      "277394 comment\n",
      "278874 link\n",
      "277840 discussion\n",
      "239180 discussion\n",
      "235708 poll\n",
      "260050 job\n"
     ]
    }
   ],
   "source": [
    "for n in [277155, 127070, 277394, 278874, 277840, 239180, 235708, 260050]:\n",
    "    url_posts = f'https://stacker.news/items/{n}'\n",
    "    response = requests.get(url_posts)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    print(n, item.detect_item_type(n, soup))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-10T07:52:27.099560322Z",
     "start_time": "2023-10-10T07:52:19.142135307Z"
    }
   },
   "id": "55c3129f32c5d366"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # Function to recursively scrape comments and capture hierarchy\n",
    "# def scrape_comments(comment_element, depth=0):\n",
    "#     comment_text = comment_element.find('div', class_='comment-text').get_text().strip()\n",
    "#     \n",
    "#     # Print the comment with proper indentation based on depth\n",
    "#     print('  ' * depth + comment_text)\n",
    "#     \n",
    "#     # Recursively scrape child comments\n",
    "#     child_comments = comment_element.find_all('div', class_='comment')\n",
    "#     for child_comment in child_comments:\n",
    "#         scrape_comments(child_comment, depth + 1)\n",
    "# \n",
    "# def scrape_post_comments(post_url):\n",
    "#     response = requests.get(post_url)\n",
    "#     if response.status_code != 200:\n",
    "#         print(\"Failed to fetch the page\")\n",
    "#         return\n",
    "# \n",
    "#     soup = BeautifulSoup(response.text, 'html.parser')\n",
    "#     \n",
    "#     # Find and scrape the post content\n",
    "#     post_content = soup.find('div', class_='post-content').get_text().strip()\n",
    "#     print(\"Post Content:\")\n",
    "#     print(post_content)\n",
    "#     \n",
    "#     # Find and scrape top-level comments\n",
    "#     top_level_comments = soup.find_all('div', class_='comment-root')\n",
    "#     \n",
    "#     print(\"\\nComments:\")\n",
    "#     for comment in top_level_comments:\n",
    "#         scrape_comments(comment)\n",
    "# \n",
    "# if __name__ == \"__main__\":\n",
    "#     post_url = \"https://example.com/post-url\"  # Replace with the URL of the post you want to scrape\n",
    "#     scrape_post_comments(post_url)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9cbb32db4d3c06ca"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Reframing the previous code\n",
    "Main steps in the code\n",
    "1. Retrieve item webpage provided the item code\n",
    "2. Detect item type:\n",
    "    - Comment or post ?\n",
    "    - If post, which kind of post:\n",
    "        1. Discussion\n",
    "        2. Link\n",
    "        3. Poll\n",
    "        4. Bounty\n",
    "        5. Job\n",
    "3. Retrieve title\n",
    "4. Retrieve banner\n",
    "    - Extract number of comment, **compulsory**\n",
    "    - Extract stacked amount by the item, **if present**\n",
    "    - Extract Boost value, **if present**\n",
    "    - Extract username, **compulsory**\n",
    "    - Extract timestamp, **compulsory**\n",
    "    - Extract badge, **compulsory**\n",
    "5. Extract amount stacked by comments, **compulsory**\n",
    "6. Extract item code of comments **OR** extract user that commented\n",
    "\n",
    "**Note that**:\n",
    "- Some items do not have the stacked amount nor the possibility to receive sats. For example the user @saloon created all this kind of posts. Is he/she a bot? Is it an 'official bot' of the forum and so it's not possible to give sats to it?\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7cd9ceda58f8bbc2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Scraping user profiles\n",
    "Users profiles are scraped starting from the list of users extracted by scraping all the items (posts+comments)\n",
    "The link to get the user profile is `https://stacker.news/$username$` "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7771202dfaced9e6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "user_list = ['Monotone',\n",
    "             'TNStacker',\n",
    "             'kale',\n",
    "             'DiracDelta',\n",
    "             'kr',\n",
    "             'moscowTimeBot',\n",
    "             'mpuels',\n",
    "             'blockstream_official',\n",
    "             'nym',\n",
    "             0,\n",
    "             'random_',\n",
    "             'saloon',\n",
    "             \"k00b\",\n",
    "             \"utente che non esiste per niente\",\n",
    "             \"DarthCoin\",\n",
    "             \"Wumbo\",\n",
    "             \"mf\",\n",
    "             \"NoStranger\",\n",
    "             \"anipy\",\n",
    "             \"OneOneSeven\",\n",
    "             \"Bitman\",\n",
    "             \"nemo\",\n",
    "             \"sahil\",\n",
    "             \"prova_di_nullo\",\n",
    "             \"babababa nullo\",\n",
    "             None,\n",
    "             ]\n",
    "\n",
    "user_list2 = [\"k00b\", \"DarthCoin\", \"saloon\"]\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "user.save_profile_csv(user_list)\n",
    "\n",
    "end = time.time()\n",
    "print(\"The provided entries are \", len(user_list),\"\\nThe average time of execution of above program for every entry is :\",\n",
    "      (end-start)/len(user_list), \"\\nThe total time of execution is \", (end-start))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "872713e01a844e3a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Modularize the code - user profile scraping\n",
    "Define functions to modularize and simplify the user profile scraping.\n",
    "The functions are defined in `user_modules/scraping_user.py` and have been tested with `tests/test_scraping_user.py`. To run the tests open a the terminal/CMD and run the script `tests/test_scraping_user.py` from there, running it from a JupyterNotebook could raise errors.\n",
    "The testing script tested every function in some corner cases (missing data/request error). "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6e52500e0ab6223e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**From the following two approaches we can create the final script for scraping the user profiles**.\n",
    "The next step would be the creation of a function that loops through the user list and assigns to the rows in the dataframe the values returned from the functions defined in `user_modules/scraping_user.py`."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "efe3fde8a3810bc4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: integrate this method into the general scraping script for profile scraping\n",
    "\n",
    "user_list = np.array(['Monotone',\n",
    "                      'TNStacker',\n",
    "                      'kale',\n",
    "                      'DiracDelta',\n",
    "                      'kr',\n",
    "                      'moscowTimeBot',\n",
    "                      'mpuels',\n",
    "                      'blockstream_official',\n",
    "                      'nym',\n",
    "                      'random_'\n",
    "                      ])\n",
    "                      \n",
    "start = time.time()\n",
    "\n",
    "for i in np.nditer(user_list):\n",
    "    print(user.get_profile(i))\n",
    "    \n",
    "end = time.time()\n",
    "print(\"The average time of execution of the above program for one user is :\",\n",
    "      (end-start)/len(user_list), \"s\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "19462e10c12c6aed"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "user_list = ['Monotone',\n",
    "             'TNStacker',\n",
    "             'kale',\n",
    "             'DiracDelta',\n",
    "             'kr',\n",
    "             'moscowTimeBot',\n",
    "             'mpuels',\n",
    "             'blockstream_official',\n",
    "             'nym',\n",
    "             'random_',\n",
    "             ]\n",
    "                      \n",
    "start = time.time()\n",
    "\n",
    "for i in user_list:\n",
    "    print(user.get_profile(i))\n",
    "    \n",
    "end = time.time()\n",
    "print(\"The average time of execution of the above program for one user is :\",\n",
    "      (end-start)/len(user_list), \"s\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "930f937ca7b5188f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
