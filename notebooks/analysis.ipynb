{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-09T11:44:07.957486705Z",
     "start_time": "2023-10-09T11:44:07.416340758Z"
    }
   },
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "from bs4 import BeautifulSoup\n",
    "import requests as requests\n",
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scripts import user\n",
    "from user_modules import scraping_item as si\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test chunck"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9c0978c01a736aa8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "titles = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for i in range(1,5):\n",
    "    \n",
    "    try:\n",
    "        url_posts = 'https://stacker.news/items/%d' % (i)\n",
    "        response = requests.get(url_posts)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "        title = soup.find_all('a', class_='item_title__FH7AS text-reset me-2')\n",
    "        \n",
    "        for e in title:\n",
    "            titles.append(e.get_text())\n",
    "        \n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "end = time.time()\n",
    "\n",
    "print(\"The time of execution of above program is :\",\n",
    "      (end-start), \"s\")\n",
    "print(titles)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c1b53dd5238f1a54"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Collect post header with all the related data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "22c54d832a42cfb6"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['El Salvador Makes Bitcoin Legal Tender', ['603 sats', ' \\\\ ', '4 boost', ' \\\\ ', ' \\\\ ', '@k00b  11 Jun 2021', ' ', '', ' ', 'bitcoin', '']]\n"
     ]
    }
   ],
   "source": [
    "url_posts = 'https://stacker.news/items/1'\n",
    "response = requests.get(url_posts)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "titolo = soup.find('a', class_='item_title__FH7AS text-reset me-2').get_text()\n",
    "banner = soup.find('div', class_='item_other__MjgP3')\n",
    "\n",
    "#post_stack = banner.find('span').text\n",
    "\n",
    "banner_data = []\n",
    "\n",
    "for i in banner.find_all('span'):\n",
    "    banner_data.append(i.text)\n",
    "    \n",
    "#n_comments = soup.find('a', class_='text-reset position-relative').get_text()\n",
    "#nym_creator = banner.find('a', ) \n",
    "\n",
    "print([titolo, banner_data])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-09T15:10:59.241912688Z",
     "start_time": "2023-10-09T15:10:58.234625911Z"
    }
   },
   "id": "83f77a4a0af8c262"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Scraping Items of stacker.news"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b0413ab8459c1374"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "  item                                         title   n_comments  \\\n0    1        El Salvador Makes Bitcoin Legal Tender  13 comments   \n1    2                  stacker news soft launch AMA  30 comments   \n7    8  The Lightning Conference 2019: Alex Bosworth   2 comments   \n\n                                              corpus    boost      sats  \\\n0                                                     4 boost  603 sats   \n1  Figured I'd make a place to raise questions ab...      NaN    2 sats   \n7                                                         NaN    0 sats   \n\n                                          commentors  \\\n0  [/k00b, /ln123, /k00b, /k00b, /OneOneSeven, /e...   \n1  [/k00b, /sha256, /k00b, /banana, /banana, /k00...   \n7         [/k00b, /k00b, /k00b, /k00b, /kr, /ekzyis]   \n\n                                      external_links author         data  \n0  bitcoinmagazine.com/business/el-salvador-appro...   k00b  11 Jun 2021  \n1  bitcoinmagazine.com/business/el-salvador-appro...   k00b  11 Jun 2021  \n7                www.youtube.com/watch?v=Y4uxMtFwaYc   k00b  11 Jun 2021  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>item</th>\n      <th>title</th>\n      <th>n_comments</th>\n      <th>corpus</th>\n      <th>boost</th>\n      <th>sats</th>\n      <th>commentors</th>\n      <th>external_links</th>\n      <th>author</th>\n      <th>data</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>El Salvador Makes Bitcoin Legal Tender</td>\n      <td>13 comments</td>\n      <td></td>\n      <td>4 boost</td>\n      <td>603 sats</td>\n      <td>[/k00b, /ln123, /k00b, /k00b, /OneOneSeven, /e...</td>\n      <td>bitcoinmagazine.com/business/el-salvador-appro...</td>\n      <td>k00b</td>\n      <td>11 Jun 2021</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>stacker news soft launch AMA</td>\n      <td>30 comments</td>\n      <td>Figured I'd make a place to raise questions ab...</td>\n      <td>NaN</td>\n      <td>2 sats</td>\n      <td>[/k00b, /sha256, /k00b, /banana, /banana, /k00...</td>\n      <td>bitcoinmagazine.com/business/el-salvador-appro...</td>\n      <td>k00b</td>\n      <td>11 Jun 2021</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>8</td>\n      <td>The Lightning Conference 2019: Alex Bosworth</td>\n      <td>2 comments</td>\n      <td></td>\n      <td>NaN</td>\n      <td>0 sats</td>\n      <td>[/k00b, /k00b, /k00b, /k00b, /kr, /ekzyis]</td>\n      <td>www.youtube.com/watch?v=Y4uxMtFwaYc</td>\n      <td>k00b</td>\n      <td>11 Jun 2021</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a list of the items to scrape\n",
    "n_posts = range(1, 10)\n",
    "\n",
    "df = pd.DataFrame(columns=['item','title', 'n_comments','corpus' ,'boost', 'sats', 'betha','commentors','external_links'])\n",
    "\n",
    "for number in n_posts:\n",
    "    \n",
    "    # Initialize the dataframe\n",
    "    url_posts = f'https://stacker.news/items/{number}'\n",
    "    response = requests.get(url_posts)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    index = number-1\n",
    "#-------------------------------------------------------------------------------\n",
    "    # Title\n",
    "    try:\n",
    "        title = soup.find('a', class_='item_title__FH7AS text-reset me-2').get_text()\n",
    "    except:\n",
    "        continue\n",
    "    # add title to dataframe\n",
    "    df.at[index, 'title'] = title\n",
    "#-------------------------------------------------------------------------------\n",
    "    # ITEM\n",
    "    df.at[index, 'item'] = number\n",
    "#-------------------------------------------------------------------------------\n",
    "    # CORPUS\n",
    "    try:\n",
    "        corpus=soup.find('div', class_=\"item_fullItemContainer__ZAYtZ\").get_text()\n",
    "    except:\n",
    "        df.at[index, 'corpus'] = \"NaN\"\n",
    "    # add title to dataframe\n",
    "    df.at[index, 'corpus'] = corpus\n",
    "#-------------------------------------------------------------------------------\n",
    "    # EXTERNAL LINKS\n",
    "    try:\n",
    "        ex_link=soup.find('a', class_=\"item_link__4cWVs\").get_text()\n",
    "    except:\n",
    "        df.at[index, 'external_links'] = \"NaN\"\n",
    "    # add external links to dataframe\n",
    "    df.at[index, 'external_links'] = ex_link\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "    # BANNER\n",
    "    try:\n",
    "        banner = soup.find('div', class_='item_other__MjgP3')\n",
    "    except:\n",
    "        df.at[index, 'boost'] = \"NaN\"\n",
    "        df.at[index, 'sats'] = \"NaN\"\n",
    "        df.at[index, 'betha'] = \"NaN\"\n",
    "    # deal with banner\n",
    "    banner_data = []\n",
    "    for i in banner.find_all('span'):\n",
    "        banner_data.append(i.text)\n",
    "    for b in banner_data:\n",
    "        if \"boost\" in b:\n",
    "            df.at[index, 'boost'] = b\n",
    "        if \"sats\" in b:\n",
    "            df.at[index, 'sats'] = b\n",
    "        if \"@\" in b:\n",
    "            df.at[index, 'betha'] = b\n",
    "#-------------------------------------------------------------------------------\n",
    "    # N_COMMENTS\n",
    "    try:\n",
    "        n_comments = soup.find('a', class_='text-reset position-relative').get_text()\n",
    "    except:        \n",
    "        df.at[index, 'n_comments'] = \"NaN\"\n",
    "    # add n_comments to dataframe\n",
    "    df.at[index, 'n_comments'] = n_comments\n",
    "#-------------------------------------------------------------------------------\n",
    "    # COMMENTORS\n",
    "    \n",
    "    a_elements = soup.find_all('a')\n",
    "    \n",
    "    df.at[index, 'commentors'] = \"NaN\"\n",
    "    at_elements = []\n",
    "\n",
    "    for el in a_elements:\n",
    "        links = el.get_text() \n",
    "        if links.startswith('@'):\n",
    "            at_elements.append(el)\n",
    "    \n",
    "    commentors_list=[]\n",
    "    for ind in range(0,len(at_elements)):\n",
    "        commentors_list.append(at_elements[ind][\"href\"])\n",
    "    # add commentors to dataframe\n",
    "    df.at[index, 'commentors'] = commentors_list\n",
    "#-------------------------------------------------------------------------------\n",
    "# Fixing the dataframe\n",
    "df['author'] = df['betha'].str.extract(r'@(\\w+)')\n",
    "df['data'] = df['betha'].str.extract(r'(\\d+\\s\\w+\\s\\d+)')\n",
    "df.drop('betha', axis=1, inplace=True)\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-09T12:54:01.162441010Z",
     "start_time": "2023-10-09T12:53:53.620766440Z"
    }
   },
   "id": "dd5f21f2f7aff4e5"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110 sats \\ 2 replies \\ @siggy47  2hFascinating article. I guess we all can relate to having experienced audience capture. I'm sure I don't even realize how it's affected me and to what extent, but I have noticed it on occasion. It's usually when I write a thoughtless, automatic post or reply, relying on groupthink rather than my own beliefs.\n",
      "Thanks for posting.\n"
     ]
    }
   ],
   "source": [
    "url_posts = 'https://stacker.news/items/278856'\n",
    "response = requests.get(url_posts)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "commenti_prova = []\n",
    "\n",
    "for i in soup.find_all('div', class_='item_item__Q_HbW comment_item__kLv_x'):\n",
    "    commenti_prova.append(i.get_text())\n",
    "    \n",
    "print(commenti_prova[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-09T16:19:35.722162234Z",
     "start_time": "2023-10-09T16:19:34.454453828Z"
    }
   },
   "id": "d39e7fd3732da90a"
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[278856, 278922, 278948, 278961]\n"
     ]
    }
   ],
   "source": [
    "# This code collects the item number of every comment in a post\n",
    "\n",
    "import re\n",
    "url_posts = 'https://stacker.news/items/278856'\n",
    "response = requests.get(url_posts)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "commenters = []\n",
    "\n",
    "for i in soup.find_all('a', class_='text-reset position-relative'):\n",
    "     c = i.get('href')\n",
    "     r = r'(\\D+)'\n",
    "     res = int(re.sub(r, '', c))\n",
    "     \n",
    "     commenters.append(res)\n",
    "      \n",
    "print(commenters)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-09T17:00:54.828858614Z",
     "start_time": "2023-10-09T17:00:54.068159368Z"
    }
   },
   "id": "a1fc059395aa864a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # Function to recursively scrape comments and capture hierarchy\n",
    "# def scrape_comments(comment_element, depth=0):\n",
    "#     comment_text = comment_element.find('div', class_='comment-text').get_text().strip()\n",
    "#     \n",
    "#     # Print the comment with proper indentation based on depth\n",
    "#     print('  ' * depth + comment_text)\n",
    "#     \n",
    "#     # Recursively scrape child comments\n",
    "#     child_comments = comment_element.find_all('div', class_='comment')\n",
    "#     for child_comment in child_comments:\n",
    "#         scrape_comments(child_comment, depth + 1)\n",
    "# \n",
    "# def scrape_post_comments(post_url):\n",
    "#     response = requests.get(post_url)\n",
    "#     if response.status_code != 200:\n",
    "#         print(\"Failed to fetch the page\")\n",
    "#         return\n",
    "# \n",
    "#     soup = BeautifulSoup(response.text, 'html.parser')\n",
    "#     \n",
    "#     # Find and scrape the post content\n",
    "#     post_content = soup.find('div', class_='post-content').get_text().strip()\n",
    "#     print(\"Post Content:\")\n",
    "#     print(post_content)\n",
    "#     \n",
    "#     # Find and scrape top-level comments\n",
    "#     top_level_comments = soup.find_all('div', class_='comment-root')\n",
    "#     \n",
    "#     print(\"\\nComments:\")\n",
    "#     for comment in top_level_comments:\n",
    "#         scrape_comments(comment)\n",
    "# \n",
    "# if __name__ == \"__main__\":\n",
    "#     post_url = \"https://example.com/post-url\"  # Replace with the URL of the post you want to scrape\n",
    "#     scrape_post_comments(post_url)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9cbb32db4d3c06ca"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Reframing the previous code\n",
    "Main steps in the code\n",
    "1. Retrieve item webpage provided the item code\n",
    "2. Detect item type:\n",
    "    - Comment or post ?\n",
    "    - If post, which kind of post:\n",
    "        1. Discussion\n",
    "        2. Link\n",
    "        3. Poll\n",
    "        4. Bounty\n",
    "        5. Job\n",
    "3. Retrieve title\n",
    "4. Retrieve banner\n",
    "    - Extract number of comment, **compulsory**\n",
    "    - Extract stacked amount by the item, **if present**\n",
    "    - Extract Boost value, **if present**\n",
    "    - Extract username, **compulsory**\n",
    "    - Extract timestamp, **compulsory**\n",
    "    - Extract badge, **compulsory**\n",
    "5. Extract amount stacked by comments, **compulsory**\n",
    "6. Extract item code of comments **OR** extract user that commented\n",
    "\n",
    "**Note that**:\n",
    "- Some items do not have the stacked amount nor the possibility to receive sats. For example the user @saloon created all this kind of posts. Is he/she a bot? Is it an 'official bot' of the forum and so it's not possible to give sats to it?\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7cd9ceda58f8bbc2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Detecting item types\n",
    "First of all we need to determine the kind of item we are working with.\n",
    "The typology is described by the tag of the post in the post-creation."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e7397c7e162d9e3d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "reload(si)\n",
    "\n",
    "NA = None\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d0864f7fbcca9cdf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Scraping user profiles\n",
    "Users profiles are scraped starting from the list of users extracted by scraping all the items (posts+comments)\n",
    "The link to get the user profile is `https://stacker.news/$username$` "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7771202dfaced9e6"
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided entries are  26 \n",
      "The average time of execution of above program for every entry is : 0.6736961236366859 \n",
      "The total time of execution is  17.516099214553833\n"
     ]
    }
   ],
   "source": [
    "user_list = ['Monotone',\n",
    "             'TNStacker',\n",
    "             'kale',\n",
    "             'DiracDelta',\n",
    "             'kr',\n",
    "             'moscowTimeBot',\n",
    "             'mpuels',\n",
    "             'blockstream_official',\n",
    "             'nym',\n",
    "             0,\n",
    "             'random_',\n",
    "             'saloon',\n",
    "             \"k00b\",\n",
    "             \"utente che non esiste per niente\",\n",
    "             \"DarthCoin\",\n",
    "             \"Wumbo\",\n",
    "             \"mf\",\n",
    "             \"NoStranger\",\n",
    "             \"anipy\",\n",
    "             \"OneOneSeven\",\n",
    "             \"Bitman\",\n",
    "             \"nemo\",\n",
    "             \"sahil\",\n",
    "             \"prova_di_nullo\",\n",
    "             \"babababa nullo\",\n",
    "             None,\n",
    "             ]\n",
    "\n",
    "user_list2 = [\"k00b\", \"DarthCoin\", \"saloon\"]\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "user.save_profile_csv(user_list)\n",
    "\n",
    "end = time.time()\n",
    "print(\"The provided entries are \", len(user_list),\"\\nThe average time of execution of above program for every entry is :\",\n",
    "      (end-start)/len(user_list), \"\\nThe total time of execution is \", (end-start))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-09T17:11:16.356334804Z",
     "start_time": "2023-10-09T17:10:58.777474744Z"
    }
   },
   "id": "872713e01a844e3a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Modularize the code - user profile scraping\n",
    "Define functions to modularize and simplify the user profile scraping.\n",
    "The functions are defined in `user_modules/scraping_user.py` and have been tested with `tests/test_scraping_user.py`. To run the tests open a the terminal/CMD and run the script `tests/test_scraping_user.py` from there, running it from a JupyterNotebook could raise errors.\n",
    "The testing script tested every function in some corner cases (missing data/request error). "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6e52500e0ab6223e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**From the following two approaches we can create the final script for scraping the user profiles**.\n",
    "The next step would be the creation of a function that loops through the user list and assigns to the rows in the dataframe the values returned from the functions defined in `user_modules/scraping_user.py`."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "efe3fde8a3810bc4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: integrate this method into the general scraping script for profile scraping\n",
    "\n",
    "user_list = np.array(['Monotone',\n",
    "                      'TNStacker',\n",
    "                      'kale',\n",
    "                      'DiracDelta',\n",
    "                      'kr',\n",
    "                      'moscowTimeBot',\n",
    "                      'mpuels',\n",
    "                      'blockstream_official',\n",
    "                      'nym',\n",
    "                      'random_'\n",
    "                      ])\n",
    "                      \n",
    "start = time.time()\n",
    "\n",
    "for i in np.nditer(user_list):\n",
    "    print(user.get_profile(i))\n",
    "    \n",
    "end = time.time()\n",
    "print(\"The average time of execution of the above program for one user is :\",\n",
    "      (end-start)/len(user_list), \"s\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "19462e10c12c6aed"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "user_list = ['Monotone',\n",
    "             'TNStacker',\n",
    "             'kale',\n",
    "             'DiracDelta',\n",
    "             'kr',\n",
    "             'moscowTimeBot',\n",
    "             'mpuels',\n",
    "             'blockstream_official',\n",
    "             'nym',\n",
    "             'random_',\n",
    "             ]\n",
    "                      \n",
    "start = time.time()\n",
    "\n",
    "for i in user_list:\n",
    "    print(user.get_profile(i))\n",
    "    \n",
    "end = time.time()\n",
    "print(\"The average time of execution of the above program for one user is :\",\n",
    "      (end-start)/len(user_list), \"s\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "930f937ca7b5188f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
