{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-11T21:36:46.718999606Z",
     "start_time": "2023-10-11T21:36:46.370978607Z"
    }
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests as requests\n",
    "import time\n",
    "import numpy as np\n",
    "from scripts import user, item, discussion, link, poll, bounty\n",
    "from tqdm import tqdm \n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Item extraction\n",
    "Main steps in the code\n",
    "1. Retrieve item webpage provided the item code\n",
    "2. Detect item type:\n",
    "    - Comment or post ?\n",
    "    - If post, which kind of post:\n",
    "        1. Discussion\n",
    "        2. Link\n",
    "        3. Poll\n",
    "        4. Bounty\n",
    "        5. Job\n",
    "3. Retrieve title\n",
    "4. Retrieve banner\n",
    "    - Extract number of comment, **compulsory**\n",
    "    - Extract stacked amount by the item, **if present**\n",
    "    - Extract Boost value, **if present**\n",
    "    - Extract username, **compulsory**\n",
    "    - Extract timestamp, **compulsory**\n",
    "    - Extract badge, **compulsory**\n",
    "5. Extract amount stacked by comments, **compulsory**\n",
    "6. Extract item code of comments **OR** extract user that commented\n",
    "\n",
    "**Note that**:\n",
    "- Some items do not have the stacked amount nor the possibility to receive sats. For example the user @saloon created all this kind of posts. Is he/she a bot? Is it an 'official bot' of the forum and so it's not possible to give sats to it?\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7cd9ceda58f8bbc2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fixing the resulting csv and the data structure\n",
    "\n",
    "### Columns to be used for all the post item scraping\n",
    "\n",
    "- Title\n",
    "- Category\n",
    "- Item code\n",
    "- Banner data\n",
    "- Main link\n",
    "- Body links\n",
    "- Sats received by comments\n",
    "- Comment item code\n",
    "\n",
    "### Columns to be used for the comment item scraping\n",
    "- Item code\n",
    "- Banner data\n",
    "- Comment item code\n",
    "\n",
    "**NB**-> the `comment item code` in Comment item table could even be deleted, we can just keep it in order to eventually see the relationship between the comment and the comments to the specific comment"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cb61276a9127df40"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Scraping comments\n",
    "\n",
    "[Useful link to duplicate rows according to the values in a row. In our situation, we need to duplicate rows according to the usernames or according to the comment item number](https://saturncloud.io/blog/splitting-and-expanding-pandas-dataframes-based-on-column-values/)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b0413ab8459c1374"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing the extraction of item type\n",
    "\n",
    "**Since job offers do not store a lot of data about interactions with users (very few comments, only a bunch of job offers into 300k items, etc) we could drop those and do not scrape them**\n",
    "In fact you can see them by simply looking at the home page of stacker.news and filter for 'job'"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bdb82e6d277cf4af"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for n in [277155, 127070, 277394, 278874, 277840, 239180, 235708, 260050]:\n",
    "    url_posts = f'https://stacker.news/items/{n}'\n",
    "    response = requests.get(url_posts)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    print(n, item.detect_item_type(n, soup))\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "55c3129f32c5d366"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Saving data in SQLite"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "90af7930b65374fc"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "table experiments already exists",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOperationalError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 5\u001B[0m\n\u001B[1;32m      2\u001B[0m conn \u001B[38;5;241m=\u001B[39m sqlite3\u001B[38;5;241m.\u001B[39mconnect(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m../data/stacker_news.sqlite\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      4\u001B[0m cur \u001B[38;5;241m=\u001B[39m conn\u001B[38;5;241m.\u001B[39mcursor()\n\u001B[0;32m----> 5\u001B[0m \u001B[43mcur\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexecute\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mCREATE TABLE experiments (name VARCHAR, description VARCHAR)\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      6\u001B[0m conn\u001B[38;5;241m.\u001B[39mcommit()\n\u001B[1;32m      8\u001B[0m conn\u001B[38;5;241m.\u001B[39mclose()\n",
      "\u001B[0;31mOperationalError\u001B[0m: table experiments already exists"
     ]
    }
   ],
   "source": [
    "# Connection creation and database creation\n",
    "conn = sqlite3.connect('../data/stacker_news.sqlite')\n",
    "\n",
    "cur = conn.cursor()\n",
    "cur.execute('CREATE TABLE experiments (name VARCHAR, description VARCHAR)')\n",
    "conn.commit()\n",
    "\n",
    "conn.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T21:36:50.956879424Z",
     "start_time": "2023-10-11T21:36:50.900398044Z"
    }
   },
   "id": "f5601fa9f343c547"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Add data to database\n",
    "cur.execute('INSERT INTO experiments (name, description) VALUES (?, ?)',\n",
    "            ('Another User', 'Another Experiment, even using \" other characters\"'))\n",
    "conn.commit()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "61fa5fa1b4b66686"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Retrieve data from database\n",
    "cur.execute('SELECT * FROM experiments')\n",
    "data = cur.fetchall()\n",
    "data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "deb422e64afb1855"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# Add constraints to datatypes\n",
    "sql_command = \"\"\"\n",
    "DROP TABLE IF EXISTS experiments;\n",
    "DROP TABLE IF EXISTS comments;\n",
    "CREATE TABLE comments (\n",
    "    ItemCode TEXT,\n",
    "    BannerData TEXT,\n",
    "    CommentsItemCode TEXT,\n",
    "    PRIMARY KEY (ItemCode));\n",
    "INSERT INTO comments (ItemCode, BannerData, CommentsItemCode) values (\"%s\", \"%s\", \"%s\");\n",
    "\"\"\" % (\"Prova1\", \"Prova2\", \"Prova3\")\n",
    "cur.executescript(sql_command)\n",
    "conn.commit()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T21:41:57.686203624Z",
     "start_time": "2023-10-11T21:41:57.623707756Z"
    }
   },
   "id": "92ad80eb21e697a3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # Retrieve data from database\n",
    "# cur.execute('SELECT * FROM experiments WHERE id=1')\n",
    "# data = cur.fetchone()\n",
    "# data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "447b7ef2c1859b15"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Scraping user profiles\n",
    "Users profiles are scraped starting from the list of users extracted by scraping all the items (posts+comments)\n",
    "The link to get the user profile is `https://stacker.news/$username$` "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7771202dfaced9e6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "user_list = ['Monotone',\n",
    "             'TNStacker',\n",
    "             'kale',\n",
    "             'DiracDelta',\n",
    "             'kr',\n",
    "             'moscowTimeBot',\n",
    "             'mpuels',\n",
    "             'blockstream_official',\n",
    "             'nym',\n",
    "             0,\n",
    "             'random_',\n",
    "             'saloon',\n",
    "             \"k00b\",\n",
    "             \"utente che non esiste per niente\",\n",
    "             \"DarthCoin\",\n",
    "             \"Wumbo\",\n",
    "             \"mf\",\n",
    "             \"NoStranger\",\n",
    "             \"anipy\",\n",
    "             \"OneOneSeven\",\n",
    "             \"Bitman\",\n",
    "             \"nemo\",\n",
    "             \"sahil\",\n",
    "             \"prova_di_nullo\",\n",
    "             \"babababa nullo\",\n",
    "             None,\n",
    "             ]\n",
    "\n",
    "user_list2 = [\"k00b\", \"DarthCoin\", \"saloon\"]\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "user.save_profile_csv(user_list)\n",
    "\n",
    "end = time.time()\n",
    "print(\"The provided entries are \", len(user_list),\"\\nThe average time of execution of above program for every entry is :\",\n",
    "      (end-start)/len(user_list), \"\\nThe total time of execution is \", (end-start))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "872713e01a844e3a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Modularize the code - user profile scraping\n",
    "Define functions to modularize and simplify the user profile scraping.\n",
    "The functions are defined in `user_modules/scraping_user.py` and have been tested with `tests/test_scraping_user.py`. To run the tests open a the terminal/CMD and run the script `tests/test_scraping_user.py` from there, running it from a JupyterNotebook could raise errors.\n",
    "The testing script tested every function in some corner cases (missing data/request error). "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6e52500e0ab6223e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**From the following two approaches we can create the final script for scraping the user profiles**.\n",
    "The next step would be the creation of a function that loops through the user list and assigns to the rows in the dataframe the values returned from the functions defined in `user_modules/scraping_user.py`."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "efe3fde8a3810bc4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: integrate this method into the general scraping script for profile scraping\n",
    "\n",
    "user_list = np.array(['Monotone',\n",
    "                      'TNStacker',\n",
    "                      'kale',\n",
    "                      'DiracDelta',\n",
    "                      'kr',\n",
    "                      'moscowTimeBot',\n",
    "                      'mpuels',\n",
    "                      'blockstream_official',\n",
    "                      'nym',\n",
    "                      'random_'\n",
    "                      ])\n",
    "                      \n",
    "start = time.time()\n",
    "\n",
    "for i in np.nditer(user_list):\n",
    "    print(user.get_profile(i))\n",
    "    \n",
    "end = time.time()\n",
    "print(\"The average time of execution of the above program for one user is :\",\n",
    "      (end-start)/len(user_list), \"s\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "19462e10c12c6aed"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "user_list = ['Monotone',\n",
    "             'TNStacker',\n",
    "             'kale',\n",
    "             'DiracDelta',\n",
    "             'kr',\n",
    "             'moscowTimeBot',\n",
    "             'mpuels',\n",
    "             'blockstream_official',\n",
    "             'nym',\n",
    "             'random_',\n",
    "             ]\n",
    "                      \n",
    "start = time.time()\n",
    "\n",
    "for i in user_list:\n",
    "    print(user.get_profile(i))\n",
    "    \n",
    "end = time.time()\n",
    "print(\"The average time of execution of the above program for one user is :\",\n",
    "      (end-start)/len(user_list), \"s\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "930f937ca7b5188f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
