{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Data collection activities\n",
    "All the data collection activities are automated using user defined functions retrievable in the folder `scripts`."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8cef29dc85632b19"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-11T16:19:11.094738288Z",
     "start_time": "2023-10-11T16:19:11.049692056Z"
    }
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests as requests\n",
    "from scripts import user, item, discussion, link, poll, bounty, comment\n",
    "import csv\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Item scraping\n",
    "The following code saves items data into a csv file, provided a range of item codes fixed by the operator.\n",
    "\n",
    "First of all we need to initialize all the files for data collection"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b87be68b26ebf12"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Initialization of csv files"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c03ba9743017eaa9"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# Discussion items\n",
    "file_path_discussion = \"../data/discussion.csv\"\n",
    "row_head_discussion = [\"Title\",\n",
    "            \"Item code\",\n",
    "            \"Banner data\",\n",
    "            \"Body links\",\n",
    "            \"Sats received by comments\",\n",
    "            \"Comments item code\",\n",
    "            ]\n",
    "    \n",
    "with open(file_path_discussion, 'w', encoding='utf_8_sig', newline=\"\") as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerow(row_head_discussion)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T16:19:12.470625264Z",
     "start_time": "2023-10-11T16:19:12.458455926Z"
    }
   },
   "id": "6564de73a2be1f53"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# Link items\n",
    "file_path_link = \"../data/link.csv\"\n",
    "row_head_link = [\"Title\",\n",
    "                 \"Item code\",\n",
    "                 \"Banner data\",\n",
    "                 \"Main link\",\n",
    "                 \"Body links\",\n",
    "                 \"Sats received by comments\",\n",
    "                 \"Comments item code\",\n",
    "                 ]\n",
    "\n",
    "with open(file_path_link, 'w', encoding='utf_8_sig', newline=\"\") as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerow(row_head_link)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T16:19:13.159961240Z",
     "start_time": "2023-10-11T16:19:13.145810763Z"
    }
   },
   "id": "889c75a273fc413b"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# Poll items\n",
    "file_path_poll = \"../data/poll.csv\"\n",
    "row_head_poll = [\"Title\",\n",
    "                 \"Item code\",\n",
    "                 \"Banner data\",\n",
    "                 \"Body links\",\n",
    "                 \"Sats received by comments\",\n",
    "                 \"Comments item code\",\n",
    "                 ]\n",
    "\n",
    "with open(file_path_poll, 'w', encoding='utf_8_sig', newline=\"\") as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerow(row_head_poll)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T16:19:13.717422019Z",
     "start_time": "2023-10-11T16:19:13.706775062Z"
    }
   },
   "id": "57dc9fa93a88ec39"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# Bounty items\n",
    "file_path_bounty = \"../data/bounty.csv\"\n",
    "row_head_bounty = [\"Title\",\n",
    "                 \"Item code\",\n",
    "                 \"Banner data\",\n",
    "                 \"Body links\",\n",
    "                 \"Sats received by comments\",\n",
    "                 \"Comments item code\",\n",
    "                 ]\n",
    "\n",
    "with open(file_path_bounty, 'w', encoding='utf_8_sig', newline=\"\") as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerow(row_head_bounty)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T16:19:14.433594849Z",
     "start_time": "2023-10-11T16:19:14.411125007Z"
    }
   },
   "id": "31c208e4ac92981b"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# Comment items\n",
    "file_path_comment = \"../data/comment.csv\"\n",
    "row_head_comment = [\"Title\",\n",
    "            \"Item code\",\n",
    "            \"Banner data\",\n",
    "            \"Body links\",\n",
    "            \"Sats received by comments\",\n",
    "            \"Comments item code\",\n",
    "            ]\n",
    "    \n",
    "with open(file_path_comment, 'w', encoding='utf_8_sig', newline=\"\") as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerow(row_head_comment)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T16:19:15.134391227Z",
     "start_time": "2023-10-11T16:19:15.124746751Z"
    }
   },
   "id": "ebcd59394d6e5494"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "# Try to scrape 150 different items rather than in 'progressive item mode'\n",
    "from random import sample\n",
    "\n",
    "sampled_items = sample([*range(1,200000)], 200)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T16:19:15.912666873Z",
     "start_time": "2023-10-11T16:19:15.894394948Z"
    }
   },
   "id": "e819de3d57c3aa0a"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[195186, 131987, 96228, 168486, 34084, 61040, 13642, 34997, 70000, 161797, 11857, 131108, 100009, 102030, 22342, 52330, 97406, 83214, 73305, 72146, 179040, 121389, 4015, 82212, 100508, 115818, 188635, 37610, 167215, 83205, 6755, 114225, 155042, 30247, 44014, 51285, 182029, 181169, 47373, 192326, 13699, 160947, 191113, 2236, 129422, 130605, 123085, 33306, 104316, 8843, 27591, 117498, 109648, 62999, 158888, 164721, 157447, 65616, 189758, 137285, 97859, 138452, 53172, 93220, 74164, 179856, 196339, 194106, 193659, 161757, 21166, 90805, 149857, 49875, 173230, 134527, 171880, 194973, 97058, 45918, 63601, 64447, 42739, 148356, 28329, 158244, 100576, 147319, 72696, 16178, 188402, 37412, 50732, 193589, 73401, 83306, 150111, 4658, 126444, 70217, 188921, 63667, 102548, 132079, 90343, 39999, 24475, 112846, 133053, 35520, 50306, 148246, 50433, 30547, 5528, 164571, 102380, 135640, 77478, 119182, 180472, 155062, 167194, 97836, 132406, 142470, 89112, 4587, 101936, 75502, 75923, 67109, 199127, 83248, 186235, 99007, 3577, 189609, 10774, 133907, 188291, 17449, 88474, 134280, 181216, 67218, 104290, 179669, 8541, 53158, 129705, 187714, 138763, 21505, 53860, 116911, 24849, 113115, 40604, 144142, 61816, 144906, 95223, 32056, 125670, 195308, 18846, 90098, 6804, 157586, 47002, 192053, 167519, 104732, 75411, 47340, 88489, 10609, 90759, 130832, 50580, 197903, 264, 97642, 164525, 159516, 112545, 39793, 114207, 6993, 19514, 125475, 161377, 171961, 147276, 165313, 183269, 153089, 196617, 143460]\n"
     ]
    }
   ],
   "source": [
    "print(sampled_items)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T16:19:16.740502163Z",
     "start_time": "2023-10-11T16:19:16.731966951Z"
    }
   },
   "id": "5622bfb2d27b3ab8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## First attempt"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c287810e4c776ebd"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [02:17<00:00,  1.46it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(sampled_items):\n",
    "    try:\n",
    "        # Provided a string returns a bs4.BeautifulSoup object\n",
    "        url_posts = f'https://stacker.news/items/{i}'\n",
    "        response = requests.get(url_posts)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        if item.detect_item_type(i, soup)=='comment':\n",
    "            entry = [None,\n",
    "                     str(i),\n",
    "                     comment.extract_banner(soup),\n",
    "                     comment.extract_body_links(soup),\n",
    "                     comment.extract_comment_stacked(soup),\n",
    "                     comment.extract_comment_item_code(soup)\n",
    "                     ]\n",
    "            \n",
    "            # Appends every new profile to a csv file in the provided path\n",
    "            try:\n",
    "                with open(file_path_comment, 'a', encoding='utf_8_sig', newline=\"\") as csvfile:\n",
    "                    csvwriter = csv.writer(csvfile)\n",
    "                    csvwriter.writerow(entry)\n",
    "            except:\n",
    "                print('Error while processing data')\n",
    "        \n",
    "        elif item.detect_item_type(i, soup)=='discussion':\n",
    "            entry = [discussion.extract_title(soup),\n",
    "                     str(i),\n",
    "                     discussion.extract_banner(soup),\n",
    "                     discussion.extract_body_links(soup),\n",
    "                     discussion.extract_comment_stacked(soup),\n",
    "                     discussion.extract_comment_item_code(soup)\n",
    "                     ]\n",
    "            \n",
    "            # Appends every new profile to a csv file in the provided path\n",
    "            try:\n",
    "                with open(file_path_discussion, 'a', encoding='utf_8_sig', newline=\"\") as csvfile:\n",
    "                    csvwriter = csv.writer(csvfile)\n",
    "                    csvwriter.writerow(entry)\n",
    "            except:\n",
    "                print('Error while processing data')\n",
    "        \n",
    "        elif item.detect_item_type(i, soup)=='link':\n",
    "            entry = [link.extract_title(soup),\n",
    "                     str(i),\n",
    "                     link.extract_banner(soup),\n",
    "                     link.extract_link(soup),\n",
    "                     link.extract_body_links(soup),\n",
    "                     link.extract_comment_stacked(soup),\n",
    "                     link.extract_comment_item_code(soup)\n",
    "                     ]\n",
    "            # Appends every new profile to a csv file in the provided path\n",
    "            try:\n",
    "                with open(file_path_link, 'a', encoding='utf_8_sig', newline=\"\") as csvfile:\n",
    "                    csvwriter = csv.writer(csvfile)\n",
    "                    csvwriter.writerow(entry)\n",
    "            except:\n",
    "                print('Error while processing data')\n",
    "                \n",
    "        elif item.detect_item_type(i, soup)=='poll':\n",
    "            entry = [poll.extract_title(soup),\n",
    "                     str(i),\n",
    "                     poll.extract_banner(soup),\n",
    "                     poll.extract_body_links(soup),\n",
    "                     poll.extract_comment_stacked(soup),\n",
    "                     poll.extract_comment_item_code(soup)\n",
    "                     ]\n",
    "            # Appends every new profile to a csv file in the provided path\n",
    "            try:\n",
    "                with open(file_path_poll, 'a', encoding='utf_8_sig', newline=\"\") as csvfile:\n",
    "                    csvwriter = csv.writer(csvfile)\n",
    "                    csvwriter.writerow(entry)\n",
    "            except:\n",
    "                print('Error while processing data')\n",
    "        \n",
    "        elif item.detect_item_type(i, soup)=='bounty':\n",
    "            entry = [bounty.extract_title(soup),\n",
    "                     str(i),\n",
    "                     bounty.extract_banner(soup),\n",
    "                     bounty.extract_body_links(soup),\n",
    "                     bounty.extract_comment_stacked(soup),\n",
    "                     bounty.extract_comment_item_code(soup)\n",
    "                     ]\n",
    "            # Appends every new profile to a csv file in the provided path\n",
    "            try:\n",
    "                with open(file_path_bounty, 'a', encoding='utf_8_sig', newline=\"\") as csvfile:\n",
    "                    csvwriter = csv.writer(csvfile)\n",
    "                    csvwriter.writerow(entry)\n",
    "            except:\n",
    "                print('Error while processing data')\n",
    "        \n",
    "    except:\n",
    "        continue\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T16:21:35.398907311Z",
     "start_time": "2023-10-11T16:19:17.922348229Z"
    }
   },
   "id": "4f0d197720b15859"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fixing the resulting csv and the data structure\n",
    "\n",
    "### Columns to be used for all the post item scraping\n",
    "\n",
    "- Title\n",
    "- Item code\n",
    "- Banner data\n",
    "- Main link\n",
    "- Body links\n",
    "- Sats received by comments\n",
    "- Comment item code\n",
    "\n",
    "### Columns to be used for the comment item scraping\n",
    "- Item code\n",
    "- Banner data\n",
    "- Comment item code\n",
    "\n",
    "**NB**-> the `comment item code` in Comment item table could even be deleted, we can just keep it in order to eventually see the relationship between the comment and the comments to the specific comment"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "342b0be052ee1177"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i in tqdm(sampled_items):\n",
    "    try:\n",
    "        # Provided a string returns a bs4.BeautifulSoup object\n",
    "        url_posts = f'https://stacker.news/items/{i}'\n",
    "        response = requests.get(url_posts)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        if item.detect_item_type(i, soup)=='comment':\n",
    "            entry = [None,\n",
    "                     str(i),\n",
    "                     comment.extract_banner(soup),\n",
    "                     comment.extract_body_links(soup),\n",
    "                     comment.extract_comment_stacked(soup),\n",
    "                     comment.extract_comment_item_code(soup)\n",
    "                     ]\n",
    "            \n",
    "            # Appends every new profile to a csv file in the provided path\n",
    "            try:\n",
    "                with open(file_path_comment, 'a', encoding='utf_8_sig', newline=\"\") as csvfile:\n",
    "                    csvwriter = csv.writer(csvfile)\n",
    "                    csvwriter.writerow(entry)\n",
    "            except:\n",
    "                print('Error while processing data')\n",
    "        \n",
    "        elif item.detect_item_type(i, soup)=='discussion':\n",
    "            entry = [discussion.extract_title(soup),\n",
    "                     str(i),\n",
    "                     discussion.extract_banner(soup),\n",
    "                     discussion.extract_body_links(soup),\n",
    "                     discussion.extract_comment_stacked(soup),\n",
    "                     discussion.extract_comment_item_code(soup)\n",
    "                     ]\n",
    "            \n",
    "            # Appends every new profile to a csv file in the provided path\n",
    "            try:\n",
    "                with open(file_path_discussion, 'a', encoding='utf_8_sig', newline=\"\") as csvfile:\n",
    "                    csvwriter = csv.writer(csvfile)\n",
    "                    csvwriter.writerow(entry)\n",
    "            except:\n",
    "                print('Error while processing data')\n",
    "        \n",
    "        elif item.detect_item_type(i, soup)=='link':\n",
    "            entry = [link.extract_title(soup),\n",
    "                     str(i),\n",
    "                     link.extract_banner(soup),\n",
    "                     link.extract_link(soup),\n",
    "                     link.extract_body_links(soup),\n",
    "                     link.extract_comment_stacked(soup),\n",
    "                     link.extract_comment_item_code(soup)\n",
    "                     ]\n",
    "            # Appends every new profile to a csv file in the provided path\n",
    "            try:\n",
    "                with open(file_path_link, 'a', encoding='utf_8_sig', newline=\"\") as csvfile:\n",
    "                    csvwriter = csv.writer(csvfile)\n",
    "                    csvwriter.writerow(entry)\n",
    "            except:\n",
    "                print('Error while processing data')\n",
    "                \n",
    "        elif item.detect_item_type(i, soup)=='poll':\n",
    "            entry = [poll.extract_title(soup),\n",
    "                     str(i),\n",
    "                     poll.extract_banner(soup),\n",
    "                     poll.extract_body_links(soup),\n",
    "                     poll.extract_comment_stacked(soup),\n",
    "                     poll.extract_comment_item_code(soup)\n",
    "                     ]\n",
    "            # Appends every new profile to a csv file in the provided path\n",
    "            try:\n",
    "                with open(file_path_poll, 'a', encoding='utf_8_sig', newline=\"\") as csvfile:\n",
    "                    csvwriter = csv.writer(csvfile)\n",
    "                    csvwriter.writerow(entry)\n",
    "            except:\n",
    "                print('Error while processing data')\n",
    "        \n",
    "        elif item.detect_item_type(i, soup)=='bounty':\n",
    "            entry = [bounty.extract_title(soup),\n",
    "                     str(i),\n",
    "                     bounty.extract_banner(soup),\n",
    "                     bounty.extract_body_links(soup),\n",
    "                     bounty.extract_comment_stacked(soup),\n",
    "                     bounty.extract_comment_item_code(soup)\n",
    "                     ]\n",
    "            # Appends every new profile to a csv file in the provided path\n",
    "            try:\n",
    "                with open(file_path_bounty, 'a', encoding='utf_8_sig', newline=\"\") as csvfile:\n",
    "                    csvwriter = csv.writer(csvfile)\n",
    "                    csvwriter.writerow(entry)\n",
    "            except:\n",
    "                print('Error while processing data')\n",
    "        \n",
    "    except:\n",
    "        continue\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2d4a7393bff6ea1a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
