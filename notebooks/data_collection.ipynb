{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Data collection activities\n",
    "All the data collection activities are automated using user defined functions retrievable in the folder `scripts`."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8cef29dc85632b19"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-11T06:20:26.853657187Z",
     "start_time": "2023-10-11T06:20:26.442344293Z"
    }
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests as requests\n",
    "from scripts import user, item, discussion\n",
    "import csv\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Discussion items\n",
    "The following code saves the discussion items data into a csv file, provided a range of item codes fixed by the operator.\n",
    "\n",
    "First of all we need to initialize all the files for data collection"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b87be68b26ebf12"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "file_path_discussion = \"../data/discussion.csv\"\n",
    "row_head_discussion = [\"Title\",\n",
    "            \"Item code\",\n",
    "            \"Banner data\",\n",
    "            \"Body links\",\n",
    "            \"Sats received by comments\",\n",
    "            \"Comments item code\",\n",
    "            ]\n",
    "    \n",
    "with open(file_path_discussion, 'w', encoding='utf_8_sig', newline=\"\") as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerow(row_head_discussion)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T06:20:30.881482645Z",
     "start_time": "2023-10-11T06:20:30.874828691Z"
    }
   },
   "id": "6564de73a2be1f53"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Try to scrape 150 different items rather than in 'progressive item mode'\n",
    "from random import sample\n",
    "\n",
    "sampled_items = sample([*range(1,200000)], 150)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T06:20:32.875578318Z",
     "start_time": "2023-10-11T06:20:32.856483420Z"
    }
   },
   "id": "e819de3d57c3aa0a"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[60852, 30727, 141119, 135201, 74587, 43310, 37643, 139840, 85041, 102936, 145130, 64314, 10312, 154808, 77468, 96489, 163958, 189717, 37057, 16543, 67977, 158433, 71592, 6767, 178705, 15959, 62838, 131430, 148079, 20347, 113037, 183793, 193808, 104980, 2806, 183829, 117334, 87730, 78903, 181007, 112217, 167858, 86570, 28219, 93936, 129856, 186637, 61919, 34284, 32047, 46496, 20365, 144555, 119204, 43061, 149761, 28849, 157570, 16936, 98405, 45101, 127382, 191179, 31390, 37027, 121484, 17256, 141055, 138534, 86451, 179431, 137431, 185651, 94509, 106753, 15123, 14819, 21238, 186663, 95359, 30246, 12096, 71173, 125275, 85478, 74808, 62620, 82445, 51172, 115889, 34083, 157563, 141479, 107110, 75639, 62313, 128159, 188151, 180339, 42751, 6249, 86561, 105884, 39909, 42235, 199542, 139991, 44857, 36249, 158357, 79797, 177707, 89989, 186253, 112155, 14642, 160415, 181476, 180751, 108913, 111744, 75344, 22632, 3323, 84090, 190821, 109451, 176994, 116618, 48292, 138286, 2012, 64946, 166739, 71017, 129961, 17352, 20088, 159107, 91223, 145389, 54595, 119305, 143446, 88996, 92992, 46312, 60257, 96223, 14447]\n"
     ]
    }
   ],
   "source": [
    "print(sampled_items)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-10T21:22:18.459431870Z",
     "start_time": "2023-10-10T21:22:18.446956916Z"
    }
   },
   "id": "5622bfb2d27b3ab8"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [02:40<00:00,  1.07s/it]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(sampled_items):\n",
    "    try:\n",
    "        # Provided a string returns a bs4.BeautifulSoup object\n",
    "        url_posts = f'https://stacker.news/items/{i}'\n",
    "        response = requests.get(url_posts)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        if item.detect_item_type(i, soup)=='discussion':\n",
    "            entry = [discussion.extract_title(soup),\n",
    "                     str(i),\n",
    "                     discussion.extract_banner(soup),\n",
    "                     discussion.extract_body_links(soup),\n",
    "                     discussion.extract_comment_stacked(soup),\n",
    "                     discussion.extract_comment_item_code(soup)\n",
    "                     ]\n",
    "            \n",
    "            # Appends every new profile to a csv file in the provided path\n",
    "            try:\n",
    "                with open(file_path_discussion, 'a', encoding='utf_8_sig', newline=\"\") as csvfile:\n",
    "                    csvwriter = csv.writer(csvfile)\n",
    "                    csvwriter.writerow(entry)\n",
    "            except:\n",
    "                print('Error while processing data')\n",
    "    except:\n",
    "        continue\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T06:23:24.249965889Z",
     "start_time": "2023-10-11T06:20:43.450910319Z"
    }
   },
   "id": "4f0d197720b15859"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [01:34<00:00,  1.60it/s]\n"
     ]
    }
   ],
   "source": [
    "# Tryout without item classification\n",
    "for i in tqdm(sampled_items):\n",
    "    try:\n",
    "        url_posts = f'https://stacker.news/items/{i}'\n",
    "        response = requests.get(url_posts)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "        entry = [discussion.extract_title(soup),\n",
    "                 str(i),\n",
    "                 discussion.extract_banner(soup),\n",
    "                 discussion.extract_body_links(soup),\n",
    "                 discussion.extract_comment_stacked(soup),\n",
    "                 discussion.extract_comment_item_code(soup)\n",
    "                 ]\n",
    "        \n",
    "        # Appends every new profile to a csv file in the provided path\n",
    "        try:\n",
    "            with open(file_path_discussion, 'a', encoding='utf_8_sig', newline=\"\") as csvfile:\n",
    "                csvwriter = csv.writer(csvfile)\n",
    "                csvwriter.writerow(entry)\n",
    "        except:\n",
    "            print('Error while processing data')\n",
    "            \n",
    "    except:\n",
    "        continue"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-10T21:23:56.204786524Z",
     "start_time": "2023-10-10T21:22:22.161452826Z"
    }
   },
   "id": "e6442601218fb484"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
