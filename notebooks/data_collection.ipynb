{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Data collection activities\n",
    "All the data collection activities are automated using user defined functions retrievable in the folder `scripts`."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8cef29dc85632b19"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import requests as requests\n",
    "from scripts import item \n",
    "from scripts import discussion \n",
    "from scripts import link\n",
    "from scripts import comment\n",
    "from scripts import user\n",
    "from tqdm import tqdm\n",
    "import sqlite3\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-27T23:11:57.198645430Z",
     "start_time": "2023-10-27T23:11:57.188396878Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Fix wrongly retrieved rows\n",
    "Some posts and comments have been wrongfully retieved, now they're missing the author and the amount of sats stacked.\n",
    "In order to fix the error a new scraping session is needed.\n",
    "The goal is:\n",
    "- Collect the wrongfully scraped items\n",
    "- Scrape them again with the proper setup and correct html tags"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "202cfd1101a4c7fa"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Comments"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3eb5bef05e1bc3d2"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Spot the wrongfully retrieved rows for the Comments table\n",
    "conn = sqlite3.connect('../data/stacker_news.sqlite')\n",
    "\n",
    "wrong_comments = \"\"\"\n",
    "SELECT *\n",
    "FROM comments\n",
    "WHERE Author=='None' AND Sats LIKE '@%';\n",
    "\"\"\"\n",
    "\n",
    "retrieve_wrong_comments = pd.read_sql(wrong_comments, conn)\n",
    "\n",
    "retrieve_wrong_comments = pd.DataFrame(retrieve_wrong_comments)\n",
    "\n",
    "conn.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-27T21:58:45.168379942Z",
     "start_time": "2023-10-27T21:58:45.061203409Z"
    }
   },
   "id": "169659a8ee44481"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "0           28\n1           37\n2           64\n3         1312\n4         1708\n         ...  \n1695    269426\n1696    269543\n1697    269544\n1698    269546\n1699    269547\nName: ItemCode, Length: 1700, dtype: object"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieve_wrong_comments['ItemCode']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-27T21:58:46.857867176Z",
     "start_time": "2023-10-27T21:58:46.790170911Z"
    }
   },
   "id": "70d707af16096c24"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Posts"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6d95606aeb50c32d"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Spot the wrongfully retrieved rows for the Post table\n",
    "conn = sqlite3.connect('../data/stacker_news.sqlite')\n",
    "\n",
    "wrong_posts = \"\"\"\n",
    "SELECT *\n",
    "FROM post\n",
    "WHERE Author=='None' AND Sats LIKE '@%';\n",
    "\"\"\"\n",
    "\n",
    "retrieve_wrong_posts = pd.read_sql(wrong_posts, conn)\n",
    "\n",
    "retrieve_wrong_posts = pd.DataFrame(retrieve_wrong_posts)\n",
    "\n",
    "conn.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-27T21:58:49.224214636Z",
     "start_time": "2023-10-27T21:58:49.177676459Z"
    }
   },
   "id": "3bbf67b24007cdf0"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "0           31\n1           34\n2           35\n3           36\n4           92\n         ...  \n1126    268075\n1127    268337\n1128    268523\n1129    268678\n1130    269031\nName: ItemCode, Length: 1131, dtype: object"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieve_wrong_posts['ItemCode']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-27T21:58:50.334416854Z",
     "start_time": "2023-10-27T21:58:50.323602769Z"
    }
   },
   "id": "c93b4ab8bbc7a674"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### General list of items that must be scraped again"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4d36537c3f91813e"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "retrieve = list(retrieve_wrong_posts['ItemCode']) + list(retrieve_wrong_comments['ItemCode'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-27T21:58:52.018475599Z",
     "start_time": "2023-10-27T21:58:52.002095084Z"
    }
   },
   "id": "79ce6f7b5caa9e4b"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Queries for entry insertion in tables\n",
    "insert_comment = \"\"\"\n",
    "INSERT OR IGNORE INTO comments (\n",
    "    ItemCode,\n",
    "    Sats,\n",
    "    Boost,\n",
    "    Comments,\n",
    "    Author,\n",
    "    Tag,\n",
    "    Timestamp,\n",
    "    CommentsItemCode\n",
    "    ) values (?, ?, ?, ?, ?, ?, ?, ?)\n",
    "\"\"\"\n",
    "\n",
    "insert_post = \"\"\"\n",
    "INSERT OR IGNORE INTO post (\n",
    "    Title,\n",
    "    Category,\n",
    "    ItemCode,\n",
    "    Sats,\n",
    "    Boost,\n",
    "    Comments,\n",
    "    Author,\n",
    "    Tag,\n",
    "    Timestamp,\n",
    "    MainLink,\n",
    "    BodyLinks,\n",
    "    SatsReceivedComments,\n",
    "    CommentsItemCode\n",
    "    ) values (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "\"\"\"\n",
    "\n",
    "insert_exception = \"\"\"\n",
    "INSERT OR IGNORE INTO exceptions (\n",
    "    RequestResult,\n",
    "    ItemCode,\n",
    "    Soup\n",
    "    ) values (?, ?, ?)\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-27T21:58:53.398127169Z",
     "start_time": "2023-10-27T21:58:53.362024507Z"
    }
   },
   "id": "bd25d902b8ebfc1a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Setup the fixing functions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cf6ab16922a13613"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "NA = None\n",
    "\n",
    "def extract_banner(page):\n",
    "\n",
    "    # Produces a dict of banner items\n",
    "    try:\n",
    "        banner = page.find('div', class_='item_other__MjgP3')\n",
    "    except:\n",
    "        banner = NA\n",
    "\n",
    "    partial_banner_data = [i.text for i in banner.find_all('span')]\n",
    "\n",
    "    final_banner = {'sats': NA,\n",
    "                    'boost': NA,\n",
    "                    'comments': NA,\n",
    "                    'author': NA,\n",
    "                    'tag': NA,\n",
    "                    'timestamp': NA,\n",
    "                    }\n",
    "\n",
    "    # Extract data in the banner\n",
    "    username_pattern = r'@([a-zA-Z0-9]+)'\n",
    "    for b in partial_banner_data:\n",
    "        if \"boost\" in b:\n",
    "            final_banner['boost'] = b\n",
    "        elif \"sats\" in b or \"sat\" in b:\n",
    "            final_banner['sats'] = b\n",
    "        elif \"@\" in b:\n",
    "            match = re.search(username_pattern, b).group(1)\n",
    "            final_banner['author'] = match\n",
    "\n",
    "    # Extract the data not extracted yet\n",
    "    try:\n",
    "        final_banner['comments'] = page.find('a', class_='text-reset position-relative').get_text()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        final_banner['tag'] = page.find('span', class_='item_newComment__HSNhq badge').get_text()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # The try except is already in function definition\n",
    "    final_banner['timestamp'] = item.get_timedate(page)\n",
    "\n",
    "    return final_banner"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a95619cf591ab7a6"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:00<00:00,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"Why Altcoins aren't copying Taproot. Bitcoin Tech Talk #244\", 'link', '34', '@satoshisuncle  14 Jun 2021', 'None', '0 comments', 'None', 'bitcoin', '2021-06-14 18:17:21', 'None', \"['https://jimmysong.substack.com/p/why-altcoins-arent-copying-taproot']\", 'None', '[]')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"Bitrefill's Work in El Salvador\", 'link', '35', '@satoshisuncle  14 Jun 2021', 'None', '0 comments', 'None', 'bitcoin', '2021-06-14 18:23:46', 'None', \"['https://twitter.com/bitrefill/status/1402624057120641036']\", 'None', '[]')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#conn = sqlite3.connect('../data/stacker_news.sqlite')\n",
    "#cur = conn.cursor()\n",
    "\n",
    "for i in tqdm(retrieve[1:3]):\n",
    "    try:\n",
    "        # Provided a string returns a bs4.BeautifulSoup object\n",
    "        url_posts = f'https://stacker.news/items/{i}'\n",
    "        response = requests.get(url_posts)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        if item.detect_item_type(i, soup) == 'comment':\n",
    "            # Insert every new entry into a new row in the provided DB\n",
    "            entry = (str(i),\n",
    "                     str(comment.extract_banner(soup)['sats']),\n",
    "                     str(comment.extract_banner(soup)['boost']),\n",
    "                     str(comment.extract_banner(soup)['comments']),\n",
    "                     str(comment.extract_banner(soup)['author']),\n",
    "                     str(comment.extract_banner(soup)['tag']),\n",
    "                     str(comment.extract_banner(soup)['timestamp']),\n",
    "                     str(comment.extract_comment_item_code(soup))\n",
    "                     )\n",
    "            try:\n",
    "                print(entry)\n",
    "            except:\n",
    "                print(f'Error while inserting the comment item {i} in the database')\n",
    "\n",
    "        elif item.detect_item_type(i, soup) == 'link':\n",
    "            # Appends every new profile to a csv file in the provided path\n",
    "            entry = (str(link.extract_title(soup)),\n",
    "                     str(item.detect_item_type(i, soup)),\n",
    "                     str(i),\n",
    "                     str(link.extract_banner(soup)['sats']),\n",
    "                     str(link.extract_banner(soup)['boost']),\n",
    "                     str(link.extract_banner(soup)['comments']),\n",
    "                     str(link.extract_banner(soup)['author']),\n",
    "                     str(link.extract_banner(soup)['tag']),\n",
    "                     str(link.extract_banner(soup)['timestamp']),\n",
    "                     str(link.extract_link(soup)),\n",
    "                     str(link.extract_body_links(soup)),\n",
    "                     str(link.extract_comment_stacked(soup)),\n",
    "                     str(link.extract_comment_item_code(soup))\n",
    "                     )\n",
    "            try:\n",
    "                print(entry)\n",
    "            except:\n",
    "                print(f'Error while inserting the link item {i} in the database')\n",
    "\n",
    "        elif item.detect_item_type(i, soup) in ['discussion', 'poll', 'bounty']:\n",
    "            entry = (str(discussion.extract_title(soup)),\n",
    "                     str(item.detect_item_type(i, soup)),\n",
    "                     str(i),\n",
    "                     str(discussion.extract_banner(soup)['sats']),\n",
    "                     str(discussion.extract_banner(soup)['boost']),\n",
    "                     str(discussion.extract_banner(soup)['comments']),\n",
    "                     str(discussion.extract_banner(soup)['author']),\n",
    "                     str(discussion.extract_banner(soup)['tag']),\n",
    "                     str(discussion.extract_banner(soup)['timestamp']),\n",
    "                     None,\n",
    "                     str(discussion.extract_body_links(soup)),\n",
    "                     str(discussion.extract_comment_stacked(soup)),\n",
    "                     str(discussion.extract_comment_item_code(soup))\n",
    "                     )\n",
    "\n",
    "            # Appends every new profile to a csv file in the provided path\n",
    "            try:\n",
    "                print(entry)\n",
    "            except:\n",
    "                print(f'Error while inserting the post item {i} in the database')\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            conn.commit()\n",
    "            time.sleep(0.5)\n",
    "            continue\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "# Final commit\n",
    "#conn.commit()\n",
    "\n",
    "# Close connection to DB\n",
    "#cur.close()\n",
    "#conn.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-27T22:02:48.810361103Z",
     "start_time": "2023-10-27T22:02:47.076444214Z"
    }
   },
   "id": "10a560fe55161c47"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Profile scraping\n",
    "\n",
    "**NB**: this code must be run after the end of the whole scraping activity because an `unique(author)` is needed in order to scrape all the user profiles in the forum. \n",
    "\n",
    "**The `unique(author)` must be the result of a `UNION ALL` between the tables.**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "45340082a7ccf465"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('../data/stacker_news.sqlite')\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT DISTINCT Author\n",
    "FROM (\n",
    "    SELECT Author\n",
    "    FROM comments\n",
    "    UNION ALL\n",
    "    SELECT Author\n",
    "    FROM post\n",
    "     );\n",
    "\"\"\"\n",
    "\n",
    "sql_query = pd.read_sql(query, conn)\n",
    "result = pd.DataFrame(sql_query,\n",
    "                      columns=['Author'])\n",
    "\n",
    "conn.close()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cdcc38ce64f686ee"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('../data/stacker_news.sqlite')\n",
    "cur = conn.cursor()\n",
    "\n",
    "sql_user = \"\"\"\n",
    "DROP TABLE IF EXISTS user;\n",
    "CREATE TABLE user (\n",
    "    User TEXT,\n",
    "    TotalStacked TEXT,\n",
    "    FirstItem TEXT,\n",
    "    HatStreak TEXT,\n",
    "    NumItems TEXT,\n",
    "    PRIMARY KEY (User))\n",
    "\"\"\"\n",
    "\n",
    "cur.executescript(sql_user)\n",
    "\n",
    "conn.commit()\n",
    "conn.close()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "df16eb4fc5e6b153"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "insert_user = \"\"\"\n",
    "INSERT INTO user (\n",
    "    User,\n",
    "    TotalStacked,\n",
    "    FirstItem,\n",
    "    HatStreak,\n",
    "    NumItems\n",
    "    ) values (?, ?, ?, ?, ?)\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ba0d83ffa888f3fe"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('../data/stacker_news.sqlite')\n",
    "cur = conn.cursor()\n",
    "\n",
    "for i in tqdm(result['Author']):\n",
    "    try:\n",
    "        profile_data = user.get_profile(i)\n",
    "        entry = (\n",
    "            str(profile_data[0]),\n",
    "            str(profile_data[1]),\n",
    "            str(profile_data[2]),\n",
    "            str(profile_data[3]),\n",
    "            str(profile_data[4])\n",
    "        )\n",
    "        try:\n",
    "            cur.execute(insert_user, entry)\n",
    "        except:\n",
    "            continue\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "conn.commit()\n",
    "cur.close()\n",
    "conn.close()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c9be74d662451a6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
