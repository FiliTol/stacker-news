{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Data collection activities\n",
    "All the data collection activities are automated using user defined functions retrievable in the folder `scripts`."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8cef29dc85632b19"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests as requests\n",
    "from scripts import user, item, discussion, link, poll, bounty, comment\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import sqlite3"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T10:31:43.724249104Z",
     "start_time": "2023-10-12T10:31:43.715455924Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Item scraping\n",
    "The following code saves items data into a csv file, provided a range of item codes fixed by the operator.\n",
    "\n",
    "First of all we need to initialize all the files for data collection"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b87be68b26ebf12"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Initialization of csv files"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c03ba9743017eaa9"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "# Post items\n",
    "file_path_post = \"../data/post.csv\"\n",
    "row_head_post = [\"Title\",\n",
    "                 \"Category\",\n",
    "                 \"Item code\",\n",
    "                 \"Sats\",\n",
    "                 \"Boost\",\n",
    "                 \"Comments\",\n",
    "                 \"Author\",\n",
    "                 \"Tag\",\n",
    "                 \"Timestamp\",\n",
    "                 \"Main link\",\n",
    "                 \"Body links\",\n",
    "                 \"Sats received by comments\",\n",
    "                 \"Comments item code\",\n",
    "                 ]\n",
    "    \n",
    "with open(file_path_post, 'w', encoding='utf_8_sig', newline=\"\") as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerow(row_head_post)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T06:25:51.151042914Z",
     "start_time": "2023-10-12T06:25:51.138072519Z"
    }
   },
   "id": "6564de73a2be1f53"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "# Comment items\n",
    "file_path_comment = \"../data/comment.csv\"\n",
    "row_head_comment = [\"Item code\",\n",
    "                    \"Sats\",\n",
    "                    \"Boost\",\n",
    "                    \"Comments\",\n",
    "                    \"Author\",\n",
    "                    \"Tag\",\n",
    "                    \"Timestamp\",\n",
    "                    \"Comments item code\",\n",
    "                    ]\n",
    "    \n",
    "with open(file_path_comment, 'w', encoding='utf_8_sig', newline=\"\") as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerow(row_head_comment)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T06:25:52.051055554Z",
     "start_time": "2023-10-12T06:25:52.026795819Z"
    }
   },
   "id": "ebcd59394d6e5494"
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "# Try to scrape 150 different items rather than in 'progressive item mode'\n",
    "from random import sample\n",
    "\n",
    "sampled_items = sample([*range(1,200000)], 10)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T06:25:52.796068052Z",
     "start_time": "2023-10-12T06:25:52.791728893Z"
    }
   },
   "id": "e819de3d57c3aa0a"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[158222, 55308, 5707, 184266, 160623, 170054, 122519, 10545, 78662, 56820]\n"
     ]
    }
   ],
   "source": [
    "print(sampled_items)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T06:25:53.455003154Z",
     "start_time": "2023-10-12T06:25:53.444021080Z"
    }
   },
   "id": "5622bfb2d27b3ab8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Csv and data structure\n",
    "\n",
    "### Columns to be used for all the post item scraping\n",
    "\n",
    "- Title\n",
    "- Category\n",
    "- Item code\n",
    "- Banner data\n",
    "- Main link\n",
    "- Body links\n",
    "- Sats received by comments\n",
    "- Comment item code\n",
    "\n",
    "### Columns to be used for the comment item scraping\n",
    "- Item code\n",
    "- Banner data\n",
    "- Comment item code\n",
    "\n",
    "**NB**-> the `comment item code` in Comment item table could even be deleted, we can just keep it in order to eventually see the relationship between the comment and the comments to the specific comment"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "342b0be052ee1177"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# for i in tqdm(sampled_items):\n",
    "#     try:\n",
    "#         # Provided a string returns a bs4.BeautifulSoup object\n",
    "#         url_posts = f'https://stacker.news/items/{i}'\n",
    "#         response = requests.get(url_posts)\n",
    "#         response.raise_for_status()\n",
    "#         soup = BeautifulSoup(response.text, 'html.parser')\n",
    "#         \n",
    "#         if item.detect_item_type(i, soup)=='comment':\n",
    "#             entry = [str(i),\n",
    "#                      comment.extract_banner(soup),\n",
    "#                      comment.extract_comment_item_code(soup)\n",
    "#                      ]\n",
    "#             \n",
    "#             # Appends every new profile to a csv file in the provided path\n",
    "#             try:\n",
    "#                 with open(file_path_comment, 'a', encoding='utf_8_sig', newline=\"\") as csvfile:\n",
    "#                     csvwriter = csv.writer(csvfile)\n",
    "#                     csvwriter.writerow(entry)\n",
    "#             except:\n",
    "#                 print('Error while processing data')\n",
    "# \n",
    "#         elif item.detect_item_type(i, soup)=='link':\n",
    "#             entry = [link.extract_title(soup),\n",
    "#                      item.detect_item_type(i,soup),\n",
    "#                      str(i),\n",
    "#                      link.extract_banner(soup),\n",
    "#                      link.extract_link(soup),\n",
    "#                      link.extract_body_links(soup),\n",
    "#                      link.extract_comment_stacked(soup),\n",
    "#                      link.extract_comment_item_code(soup)\n",
    "#                      ]\n",
    "#             # Appends every new profile to a csv file in the provided path\n",
    "#             try:\n",
    "#                 with open(file_path_post, 'a', encoding='utf_8_sig', newline=\"\") as csvfile:\n",
    "#                     csvwriter = csv.writer(csvfile)\n",
    "#                     csvwriter.writerow(entry)\n",
    "#             except:\n",
    "#                 print('Error while processing data')\n",
    "#         \n",
    "#         elif item.detect_item_type(i, soup)=='discussion':\n",
    "#             entry = [discussion.extract_title(soup),\n",
    "#                      item.detect_item_type(i,soup),\n",
    "#                      str(i),\n",
    "#                      discussion.extract_banner(soup),\n",
    "#                      None,\n",
    "#                      discussion.extract_body_links(soup),\n",
    "#                      discussion.extract_comment_stacked(soup),\n",
    "#                      discussion.extract_comment_item_code(soup)\n",
    "#                      ]\n",
    "#             \n",
    "#             # Appends every new profile to a csv file in the provided path\n",
    "#             try:\n",
    "#                 with open(file_path_post, 'a', encoding='utf_8_sig', newline=\"\") as csvfile:\n",
    "#                     csvwriter = csv.writer(csvfile)\n",
    "#                     csvwriter.writerow(entry)\n",
    "#             except:\n",
    "#                 print('Error while processing data')\n",
    "#                         \n",
    "#         elif item.detect_item_type(i, soup)=='poll':\n",
    "#             entry = [poll.extract_title(soup),\n",
    "#                      item.detect_item_type(i, soup),\n",
    "#                      str(i),\n",
    "#                      poll.extract_banner(soup),\n",
    "#                      None,\n",
    "#                      poll.extract_body_links(soup),\n",
    "#                      poll.extract_comment_stacked(soup),\n",
    "#                      poll.extract_comment_item_code(soup)\n",
    "#                      ]\n",
    "#             # Appends every new profile to a csv file in the provided path\n",
    "#             try:\n",
    "#                 with open(file_path_post, 'a', encoding='utf_8_sig', newline=\"\") as csvfile:\n",
    "#                     csvwriter = csv.writer(csvfile)\n",
    "#                     csvwriter.writerow(entry)\n",
    "#             except:\n",
    "#                 print('Error while processing data')\n",
    "#         \n",
    "#         elif item.detect_item_type(i, soup)=='bounty':\n",
    "#             entry = [bounty.extract_title(soup),\n",
    "#                      item.detect_item_type(i, soup),\n",
    "#                      str(i),\n",
    "#                      bounty.extract_banner(soup),\n",
    "#                      None,\n",
    "#                      bounty.extract_body_links(soup),\n",
    "#                      bounty.extract_comment_stacked(soup),\n",
    "#                      bounty.extract_comment_item_code(soup)\n",
    "#                      ]\n",
    "#             # Appends every new profile to a csv file in the provided path\n",
    "#             try:\n",
    "#                 with open(file_path_post, 'a', encoding='utf_8_sig', newline=\"\") as csvfile:\n",
    "#                     csvwriter = csv.writer(csvfile)\n",
    "#                     csvwriter.writerow(entry)\n",
    "#             except:\n",
    "#                 print('Error while processing data')\n",
    "#         \n",
    "#     except:\n",
    "#         continue\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2d4a7393bff6ea1a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Optimizing for the similarities between item post types\n",
    "\n",
    "In the following chunck the functions applied are clustered for similarities, leading to three blocks:\n",
    "- Comment\n",
    "- Link\n",
    "- Discussion/poll/bounty\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "37b9b2089d27f466"
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:09<00:00,  1.01it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(sampled_items):\n",
    "    try:\n",
    "        # Provided a string returns a bs4.BeautifulSoup object\n",
    "        url_posts = f'https://stacker.news/items/{i}'\n",
    "        response = requests.get(url_posts)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        if item.detect_item_type(i, soup)=='comment':\n",
    "            entry = [str(i),\n",
    "                     comment.extract_banner(soup)['sats'],\n",
    "                     comment.extract_banner(soup)['boost'],\n",
    "                     comment.extract_banner(soup)['comments'],\n",
    "                     comment.extract_banner(soup)['author'],\n",
    "                     comment.extract_banner(soup)['tag'],\n",
    "                     comment.extract_banner(soup)['timestamp'],\n",
    "                     comment.extract_comment_item_code(soup)\n",
    "                     ]\n",
    "            \n",
    "            # Appends every new profile to a csv file in the provided path\n",
    "            try:\n",
    "                with open(file_path_comment, 'a', encoding='utf_8_sig', newline=\"\") as csvfile:\n",
    "                    csvwriter = csv.writer(csvfile)\n",
    "                    csvwriter.writerow(entry)\n",
    "            except:\n",
    "                print('Error while processing data')\n",
    "\n",
    "        elif item.detect_item_type(i, soup)=='link':\n",
    "            entry = [link.extract_title(soup),\n",
    "                     item.detect_item_type(i,soup),\n",
    "                     str(i),\n",
    "                     link.extract_banner(soup)['sats'],\n",
    "                     link.extract_banner(soup)['boost'],\n",
    "                     link.extract_banner(soup)['comments'],\n",
    "                     link.extract_banner(soup)['author'],\n",
    "                     link.extract_banner(soup)['tag'],\n",
    "                     link.extract_banner(soup)['timestamp'],\n",
    "                     link.extract_link(soup),\n",
    "                     link.extract_body_links(soup),\n",
    "                     link.extract_comment_stacked(soup),\n",
    "                     link.extract_comment_item_code(soup)\n",
    "                     ]\n",
    "            # Appends every new profile to a csv file in the provided path\n",
    "            try:\n",
    "                with open(file_path_post, 'a', encoding='utf_8_sig', newline=\"\") as csvfile:\n",
    "                    csvwriter = csv.writer(csvfile)\n",
    "                    csvwriter.writerow(entry)\n",
    "            except:\n",
    "                print('Error while processing data')\n",
    "        \n",
    "        elif item.detect_item_type(i, soup) in ['discussion', 'poll', 'bounty']:\n",
    "            entry = [discussion.extract_title(soup),\n",
    "                     item.detect_item_type(i,soup),\n",
    "                     str(i),\n",
    "                     discussion.extract_banner(soup)['sats'],\n",
    "                     discussion.extract_banner(soup)['boost'],\n",
    "                     discussion.extract_banner(soup)['comments'],\n",
    "                     discussion.extract_banner(soup)['author'],\n",
    "                     discussion.extract_banner(soup)['tag'],\n",
    "                     discussion.extract_banner(soup)['timestamp'],\n",
    "                     None,\n",
    "                     discussion.extract_body_links(soup),\n",
    "                     discussion.extract_comment_stacked(soup),\n",
    "                     discussion.extract_comment_item_code(soup)\n",
    "                     ]\n",
    "            \n",
    "            # Appends every new profile to a csv file in the provided path\n",
    "            try:\n",
    "                with open(file_path_post, 'a', encoding='utf_8_sig', newline=\"\") as csvfile:\n",
    "                    csvwriter = csv.writer(csvfile)\n",
    "                    csvwriter.writerow(entry)\n",
    "            except:\n",
    "                print('Error while processing data')\n",
    "        \n",
    "    except:\n",
    "        continue\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T06:26:05.153275386Z",
     "start_time": "2023-10-12T06:25:55.282268899Z"
    }
   },
   "id": "218bc6335159e0fb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "--------------------------------------------------------------------------------\n",
    "## Saving with SQLite "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6698982d24e26bf9"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Try to scrape 150 different items rather than in 'progressive item mode'\n",
    "from random import sample\n",
    "\n",
    "sampled_items = sample([*range(1,250000)], 5000)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T10:31:46.505537704Z",
     "start_time": "2023-10-12T10:31:46.488772319Z"
    }
   },
   "id": "407844d3e7ee49b0"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('../data/stacker_news.sqlite')\n",
    "cur = conn.cursor()\n",
    "\n",
    "sql_comment = \"\"\"\n",
    "DROP TABLE IF EXISTS comments;\n",
    "CREATE TABLE comments (\n",
    "    ItemCode TEXT,\n",
    "    Sats TEXT,\n",
    "    Boost TEXT,\n",
    "    Comments TEXT,\n",
    "    Author TEXT,\n",
    "    Tag TEXT,\n",
    "    Timestamp TEXT,\n",
    "    CommentsItemCode TEXT,\n",
    "    PRIMARY KEY (ItemCode))\n",
    "\"\"\"\n",
    "\n",
    "sql_post = \"\"\"\n",
    "DROP TABLE IF EXISTS post;\n",
    "CREATE TABLE post (\n",
    "    Title TEXT,\n",
    "    Category TEXT,\n",
    "    ItemCode TEXT,\n",
    "    Sats TEXT,\n",
    "    Boost TEXT,\n",
    "    Comments TEXT,\n",
    "    Author TEXT,\n",
    "    Tag TEXT,\n",
    "    Timestamp TEXT,\n",
    "    MainLink TEXT,\n",
    "    BodyLinks TEXT,\n",
    "    SatsReceivedComments TEXT,\n",
    "    CommentsItemCode TEXT,\n",
    "    PRIMARY KEY (ItemCode))\n",
    "\"\"\"\n",
    "\n",
    "cur.executescript(sql_comment)\n",
    "cur.executescript(sql_post)\n",
    "\n",
    "conn.commit()\n",
    "conn.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T10:31:47.343379478Z",
     "start_time": "2023-10-12T10:31:47.321924448Z"
    }
   },
   "id": "70451eecccab5a1c"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "insert_comment = \"\"\"\n",
    "INSERT INTO comments (\n",
    "    ItemCode,\n",
    "    Sats,\n",
    "    Boost,\n",
    "    Comments,\n",
    "    Author,\n",
    "    Tag,\n",
    "    Timestamp,\n",
    "    CommentsItemCode\n",
    "    ) values (?, ?, ?, ?, ?, ?, ?, ?)\n",
    "\"\"\"\n",
    "\n",
    "insert_post = \"\"\"\n",
    "INSERT INTO post (\n",
    "    Title,\n",
    "    Category,\n",
    "    ItemCode,\n",
    "    Sats,\n",
    "    Boost,\n",
    "    Comments,\n",
    "    Author,\n",
    "    Tag,\n",
    "    Timestamp,\n",
    "    MainLink,\n",
    "    BodyLinks,\n",
    "    SatsReceivedComments,\n",
    "    CommentsItemCode\n",
    "    ) values (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T10:31:48.126891563Z",
     "start_time": "2023-10-12T10:31:48.118663433Z"
    }
   },
   "id": "bd7e16d6db8f28c2"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▎      | 13/40 [00:09<00:18,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:30<00:00,  1.30it/s]\n"
     ]
    }
   ],
   "source": [
    "# conn = sqlite3.connect('../data/stacker_news.sqlite')\n",
    "# cur = conn.cursor()\n",
    "# \n",
    "# for i in tqdm(sampled_items):\n",
    "#     try:\n",
    "#         # Provided a string returns a bs4.BeautifulSoup object\n",
    "#         url_posts = f'https://stacker.news/items/{i}'\n",
    "#         response = requests.get(url_posts)\n",
    "#         response.raise_for_status()\n",
    "#         soup = BeautifulSoup(response.text, 'html.parser')\n",
    "#         \n",
    "#         if item.detect_item_type(i, soup)=='comment':\n",
    "#             # Insert every new entry into a new row in the provided DB\n",
    "#             entry = (str(i),\n",
    "#                      str(comment.extract_banner(soup)['sats']),\n",
    "#                      str(comment.extract_banner(soup)['boost']),\n",
    "#                      str(comment.extract_banner(soup)['comments']),\n",
    "#                      str(comment.extract_banner(soup)['author']),\n",
    "#                      str(comment.extract_banner(soup)['tag']),\n",
    "#                      str(comment.extract_banner(soup)['timestamp']),\n",
    "#                      str(comment.extract_comment_item_code(soup))\n",
    "#                      )\n",
    "#             try:\n",
    "#                 cur.execute(insert_comment, entry)\n",
    "#             except:\n",
    "#                 print('Error while processing item ', i)\n",
    "#     except:\n",
    "#         print(i)\n",
    "#                 \n",
    "# conn.commit()\n",
    "# cur.close()\n",
    "# conn.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T07:10:51.408681093Z",
     "start_time": "2023-10-12T07:10:20.532866063Z"
    }
   },
   "id": "3056299b445cd823"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that every entry in the sql statement is transformed into a string, only the `None` values (`NULL` in SQL) are left as is."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "57d4c8de845d26ff"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [59:01<00:00,  1.41it/s]  \n"
     ]
    }
   ],
   "source": [
    "conn = sqlite3.connect('../data/stacker_news.sqlite')\n",
    "cur = conn.cursor()\n",
    "\n",
    "for i in tqdm(sampled_items):\n",
    "    try:\n",
    "        # Provided a string returns a bs4.BeautifulSoup object\n",
    "        url_posts = f'https://stacker.news/items/{i}'\n",
    "        response = requests.get(url_posts)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        if item.detect_item_type(i, soup)=='comment':\n",
    "            # Insert every new entry into a new row in the provided DB\n",
    "            entry = (str(i),\n",
    "                     str(comment.extract_banner(soup)['sats']),\n",
    "                     str(comment.extract_banner(soup)['boost']),\n",
    "                     str(comment.extract_banner(soup)['comments']),\n",
    "                     str(comment.extract_banner(soup)['author']),\n",
    "                     str(comment.extract_banner(soup)['tag']),\n",
    "                     str(comment.extract_banner(soup)['timestamp']),\n",
    "                     str(comment.extract_comment_item_code(soup))\n",
    "                     )\n",
    "            try:\n",
    "                cur.execute(insert_comment, entry)\n",
    "            except:\n",
    "                print('Error while processing item ', i)\n",
    "\n",
    "        elif item.detect_item_type(i, soup)=='link':\n",
    "            # Appends every new profile to a csv file in the provided path\n",
    "            entry = (str(link.extract_title(soup)),\n",
    "                     str(item.detect_item_type(i,soup)),\n",
    "                     str(i),\n",
    "                     str(link.extract_banner(soup)['sats']),\n",
    "                     str(link.extract_banner(soup)['boost']),\n",
    "                     str(link.extract_banner(soup)['comments']),\n",
    "                     str(link.extract_banner(soup)['author']),\n",
    "                     str(link.extract_banner(soup)['tag']),\n",
    "                     str(link.extract_banner(soup)['timestamp']),\n",
    "                     str(link.extract_link(soup)),\n",
    "                     str(link.extract_body_links(soup)),\n",
    "                     str(link.extract_comment_stacked(soup)),\n",
    "                     str(link.extract_comment_item_code(soup))\n",
    "                     )\n",
    "            try:\n",
    "                cur.execute(insert_post, entry)\n",
    "            except:\n",
    "                print('Error while processing item ', i)\n",
    "        \n",
    "        elif item.detect_item_type(i, soup) in ['discussion', 'poll', 'bounty']:\n",
    "            entry = (str(discussion.extract_title(soup)),\n",
    "                     str(item.detect_item_type(i,soup)),\n",
    "                     str(i),\n",
    "                     str(discussion.extract_banner(soup)['sats']),\n",
    "                     str(discussion.extract_banner(soup)['boost']),\n",
    "                     str(discussion.extract_banner(soup)['comments']),\n",
    "                     str(discussion.extract_banner(soup)['author']),\n",
    "                     str(discussion.extract_banner(soup)['tag']),\n",
    "                     str(discussion.extract_banner(soup)['timestamp']),\n",
    "                     None,\n",
    "                     str(discussion.extract_body_links(soup)),\n",
    "                     str(discussion.extract_comment_stacked(soup)),\n",
    "                     str(discussion.extract_comment_item_code(soup))\n",
    "                     )\n",
    "            \n",
    "            # Appends every new profile to a csv file in the provided path\n",
    "            try:\n",
    "                cur.execute(insert_post, entry)\n",
    "            except:\n",
    "                print('Error while processing item ', i)\n",
    "        \n",
    "    except:\n",
    "        continue\n",
    "\n",
    "conn.commit()\n",
    "cur.close()\n",
    "conn.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T11:30:51.734192075Z",
     "start_time": "2023-10-12T10:31:49.676765982Z"
    }
   },
   "id": "98a83436350bc35c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
