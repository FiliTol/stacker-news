{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Data collection activities\n",
    "All the data collection activities are automated using user defined functions retrievable in the folder `scripts`."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8cef29dc85632b19"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-11T11:19:59.508339685Z",
     "start_time": "2023-10-11T11:19:59.491698097Z"
    }
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests as requests\n",
    "from scripts import user, item, discussion, link, poll, bounty\n",
    "import csv\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Item scraping\n",
    "The following code saves items data into a csv file, provided a range of item codes fixed by the operator.\n",
    "\n",
    "First of all we need to initialize all the files for data collection"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b87be68b26ebf12"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Initialization of csv files"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c03ba9743017eaa9"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# Discussion items\n",
    "file_path_discussion = \"../data/discussion.csv\"\n",
    "row_head_discussion = [\"Title\",\n",
    "            \"Item code\",\n",
    "            \"Banner data\",\n",
    "            \"Main link\",\n",
    "            \"Body links\"\n",
    "            \"Sats received by comments\",\n",
    "            \"Comments item code\",\n",
    "            ]\n",
    "    \n",
    "with open(file_path_discussion, 'w', encoding='utf_8_sig', newline=\"\") as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerow(row_head_discussion)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T11:20:00.699660654Z",
     "start_time": "2023-10-11T11:20:00.670153364Z"
    }
   },
   "id": "6564de73a2be1f53"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# Link items\n",
    "file_path_link = \"../data/link.csv\"\n",
    "row_head_link = [\"Title\",\n",
    "                 \"Item code\",\n",
    "                 \"Banner data\",\n",
    "                 \"Main link\",\n",
    "                 \"Body links\",\n",
    "                 \"Sats received by comments\",\n",
    "                 \"Comments item code\",\n",
    "                 ]\n",
    "\n",
    "with open(file_path_link, 'w', encoding='utf_8_sig', newline=\"\") as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerow(row_head_link)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T11:20:01.257874659Z",
     "start_time": "2023-10-11T11:20:01.233715004Z"
    }
   },
   "id": "889c75a273fc413b"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# Poll items\n",
    "file_path_poll = \"../data/poll.csv\"\n",
    "row_head_poll = [\"Title\",\n",
    "                 \"Item code\",\n",
    "                 \"Banner data\",\n",
    "                 \"Body links\",\n",
    "                 \"Sats received by comments\",\n",
    "                 \"Comments item code\",\n",
    "                 ]\n",
    "\n",
    "with open(file_path_poll, 'w', encoding='utf_8_sig', newline=\"\") as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerow(row_head_poll)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T11:20:02.005285976Z",
     "start_time": "2023-10-11T11:20:01.898528524Z"
    }
   },
   "id": "57dc9fa93a88ec39"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# Bounty items\n",
    "file_path_bounty = \"../data/bounty.csv\"\n",
    "row_head_bounty = [\"Title\",\n",
    "                 \"Item code\",\n",
    "                 \"Banner data\",\n",
    "                 \"Body links\",\n",
    "                 \"Sats received by comments\",\n",
    "                 \"Comments item code\",\n",
    "                 ]\n",
    "\n",
    "with open(file_path_bounty, 'w', encoding='utf_8_sig', newline=\"\") as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerow(row_head_bounty)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T11:20:02.517663002Z",
     "start_time": "2023-10-11T11:20:02.496013012Z"
    }
   },
   "id": "31c208e4ac92981b"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# Try to scrape 150 different items rather than in 'progressive item mode'\n",
    "from random import sample\n",
    "\n",
    "sampled_items = sample([*range(1,200000)], 200)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T11:20:27.320168473Z",
     "start_time": "2023-10-11T11:20:27.274191336Z"
    }
   },
   "id": "e819de3d57c3aa0a"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4122, 101221, 119598, 45764, 25264, 165557, 67921, 66819, 173882, 137321, 195291, 117750, 50240, 8846, 133159, 91006, 120190, 14819, 123144, 173231, 173563, 55963, 195912, 160645, 197813, 14092, 12903, 141529, 122120, 14060, 85310, 53511, 130617, 151593, 68220, 48864, 67804, 168603, 115590, 58944, 196704, 3349, 175045, 174626, 114051, 189467, 43844, 189433, 51442, 35397, 132747, 89567, 67741, 95651, 109942, 85799, 182477, 165126, 68575, 180039, 167063, 103776, 20753, 16515, 15962, 40173, 184196, 3601, 176378, 186725, 22257, 110766, 101046, 99426, 198485, 160150, 120309, 17986, 63270, 109122, 168077, 160886, 157929, 16089, 34721, 131683, 39687, 11549, 151099, 125437, 183017, 9432, 13034, 111302, 184927, 75044, 189457, 89402, 27708, 137628, 114098, 184876, 151194, 88696, 62230, 92218, 176718, 171402, 88079, 172689, 18559, 26050, 59447, 69385, 142337, 1384, 102788, 23991, 40962, 140192, 61835, 15391, 180066, 144121, 103063, 99926, 77336, 158473, 124152, 47432, 157672, 60727, 152227, 80909, 28988, 184344, 80829, 123110, 160359, 186844, 38737, 5549, 174755, 167130, 172772, 196909, 185331, 66409, 95732, 190954, 151675, 194558, 169041, 61051, 71701, 186869, 50170, 92890, 67126, 157567, 45541, 74803, 165883, 21285, 167009, 36440, 53568, 87611, 27903, 59470, 161388, 43633, 144013, 185374, 139563, 134456, 31267, 51949, 33659, 195925, 167585, 132831, 40849, 166985, 187684, 545, 123519, 56230, 141679, 168732, 115229, 61488, 21158, 73974, 64880, 160007, 17560, 49405, 42997, 38928]\n"
     ]
    }
   ],
   "source": [
    "print(sampled_items)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T11:20:28.069169095Z",
     "start_time": "2023-10-11T11:20:28.040834937Z"
    }
   },
   "id": "5622bfb2d27b3ab8"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [02:27<00:00,  1.36it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(sampled_items):\n",
    "    try:\n",
    "        # Provided a string returns a bs4.BeautifulSoup object\n",
    "        url_posts = f'https://stacker.news/items/{i}'\n",
    "        response = requests.get(url_posts)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        if item.detect_item_type(i, soup)=='discussion':\n",
    "            entry = [discussion.extract_title(soup),\n",
    "                     str(i),\n",
    "                     discussion.extract_banner(soup),\n",
    "                     discussion.extract_body_links(soup),\n",
    "                     discussion.extract_comment_stacked(soup),\n",
    "                     discussion.extract_comment_item_code(soup)\n",
    "                     ]\n",
    "            \n",
    "            # Appends every new profile to a csv file in the provided path\n",
    "            try:\n",
    "                with open(file_path_discussion, 'a', encoding='utf_8_sig', newline=\"\") as csvfile:\n",
    "                    csvwriter = csv.writer(csvfile)\n",
    "                    csvwriter.writerow(entry)\n",
    "            except:\n",
    "                print('Error while processing data')\n",
    "        \n",
    "        elif item.detect_item_type(i, soup)=='link':\n",
    "            entry = [link.extract_title(soup),\n",
    "                     str(i),\n",
    "                     link.extract_banner(soup),\n",
    "                     link.extract_link(soup),\n",
    "                     link.extract_body_links(soup),\n",
    "                     link.extract_comment_stacked(soup),\n",
    "                     link.extract_comment_item_code(soup)\n",
    "                     ]\n",
    "            # Appends every new profile to a csv file in the provided path\n",
    "            try:\n",
    "                with open(file_path_link, 'a', encoding='utf_8_sig', newline=\"\") as csvfile:\n",
    "                    csvwriter = csv.writer(csvfile)\n",
    "                    csvwriter.writerow(entry)\n",
    "            except:\n",
    "                print('Error while processing data')\n",
    "                \n",
    "        elif item.detect_item_type(i, soup)=='poll':\n",
    "            entry = [poll.extract_title(soup),\n",
    "                     str(i),\n",
    "                     poll.extract_banner(soup),\n",
    "                     poll.extract_body_links(soup),\n",
    "                     poll.extract_comment_stacked(soup),\n",
    "                     poll.extract_comment_item_code(soup)\n",
    "                     ]\n",
    "            # Appends every new profile to a csv file in the provided path\n",
    "            try:\n",
    "                with open(file_path_poll, 'a', encoding='utf_8_sig', newline=\"\") as csvfile:\n",
    "                    csvwriter = csv.writer(csvfile)\n",
    "                    csvwriter.writerow(entry)\n",
    "            except:\n",
    "                print('Error while processing data')\n",
    "        \n",
    "        elif item.detect_item_type(i, soup)=='bounty':\n",
    "            entry = [bounty.extract_title(soup),\n",
    "                     str(i),\n",
    "                     bounty.extract_banner(soup),\n",
    "                     bounty.extract_body_links(soup),\n",
    "                     bounty.extract_comment_stacked(soup),\n",
    "                     bounty.extract_comment_item_code(soup)\n",
    "                     ]\n",
    "            # Appends every new profile to a csv file in the provided path\n",
    "            try:\n",
    "                with open(file_path_bounty, 'a', encoding='utf_8_sig', newline=\"\") as csvfile:\n",
    "                    csvwriter = csv.writer(csvfile)\n",
    "                    csvwriter.writerow(entry)\n",
    "            except:\n",
    "                print('Error while processing data')\n",
    "        \n",
    "    except:\n",
    "        continue\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T11:22:58.771995467Z",
     "start_time": "2023-10-11T11:20:31.218162320Z"
    }
   },
   "id": "4f0d197720b15859"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
