{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Data collection activities\n",
    "All the data collection activities are automated using user defined functions retrievable in the folder `scripts`."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8cef29dc85632b19"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests as requests\n",
    "from scripts import item, discussion, link, comment, user\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import sqlite3\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T15:08:39.422030357Z",
     "start_time": "2023-10-13T15:08:38.480512716Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Item scraping\n",
    "The following code saves items data into a csv file, provided a range of item codes fixed by the operator.\n",
    "\n",
    "First of all we need to initialize all the files for data collection"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b87be68b26ebf12"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Initialization of csv files"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c03ba9743017eaa9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Post items\n",
    "file_path_post = \"../data/post.csv\"\n",
    "row_head_post = [\"Title\",\n",
    "                 \"Category\",\n",
    "                 \"Item code\",\n",
    "                 \"Sats\",\n",
    "                 \"Boost\",\n",
    "                 \"Comments\",\n",
    "                 \"Author\",\n",
    "                 \"Tag\",\n",
    "                 \"Timestamp\",\n",
    "                 \"Main link\",\n",
    "                 \"Body links\",\n",
    "                 \"Sats received by comments\",\n",
    "                 \"Comments item code\",\n",
    "                 ]\n",
    "    \n",
    "with open(file_path_post, 'w', encoding='utf_8_sig', newline=\"\") as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerow(row_head_post)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6564de73a2be1f53"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Comment items\n",
    "file_path_comment = \"../data/comment.csv\"\n",
    "row_head_comment = [\"Item code\",\n",
    "                    \"Sats\",\n",
    "                    \"Boost\",\n",
    "                    \"Comments\",\n",
    "                    \"Author\",\n",
    "                    \"Tag\",\n",
    "                    \"Timestamp\",\n",
    "                    \"Comments item code\",\n",
    "                    ]\n",
    "    \n",
    "with open(file_path_comment, 'w', encoding='utf_8_sig', newline=\"\") as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerow(row_head_comment)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ebcd59394d6e5494"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Try to scrape 150 different items rather than in 'progressive item mode'\n",
    "from random import sample\n",
    "\n",
    "sampled_items = sample([*range(1,200000)], 10)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e819de3d57c3aa0a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(sampled_items)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5622bfb2d27b3ab8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Csv and data structure\n",
    "\n",
    "### Columns to be used for all the post item scraping\n",
    "\n",
    "- Title\n",
    "- Category\n",
    "- Item code\n",
    "- Banner data\n",
    "- Main link\n",
    "- Body links\n",
    "- Sats received by comments\n",
    "- Comment item code\n",
    "\n",
    "### Columns to be used for the comment item scraping\n",
    "- Item code\n",
    "- Banner data\n",
    "- Comment item code\n",
    "\n",
    "**NB**-> the `comment item code` in Comment item table could even be deleted, we can just keep it in order to eventually see the relationship between the comment and the comments to the specific comment"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "342b0be052ee1177"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Optimizing for the similarities between item post types\n",
    "\n",
    "In the following chunck the functions applied are clustered for similarities, leading to three blocks:\n",
    "- Comment\n",
    "- Link\n",
    "- Discussion/poll/bounty\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "37b9b2089d27f466"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i in tqdm(sampled_items):\n",
    "    try:\n",
    "        # Provided a string returns a bs4.BeautifulSoup object\n",
    "        url_posts = f'https://stacker.news/items/{i}'\n",
    "        response = requests.get(url_posts)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        if item.detect_item_type(i, soup)=='comment':\n",
    "            entry = [str(i),\n",
    "                     comment.extract_banner(soup)['sats'],\n",
    "                     comment.extract_banner(soup)['boost'],\n",
    "                     comment.extract_banner(soup)['comments'],\n",
    "                     comment.extract_banner(soup)['author'],\n",
    "                     comment.extract_banner(soup)['tag'],\n",
    "                     comment.extract_banner(soup)['timestamp'],\n",
    "                     comment.extract_comment_item_code(soup)\n",
    "                     ]\n",
    "            \n",
    "            # Appends every new profile to a csv file in the provided path\n",
    "            try:\n",
    "                with open(file_path_comment, 'a', encoding='utf_8_sig', newline=\"\") as csvfile:\n",
    "                    csvwriter = csv.writer(csvfile)\n",
    "                    csvwriter.writerow(entry)\n",
    "            except:\n",
    "                print('Error while processing data')\n",
    "\n",
    "        elif item.detect_item_type(i, soup)=='link':\n",
    "            entry = [link.extract_title(soup),\n",
    "                     item.detect_item_type(i,soup),\n",
    "                     str(i),\n",
    "                     link.extract_banner(soup)['sats'],\n",
    "                     link.extract_banner(soup)['boost'],\n",
    "                     link.extract_banner(soup)['comments'],\n",
    "                     link.extract_banner(soup)['author'],\n",
    "                     link.extract_banner(soup)['tag'],\n",
    "                     link.extract_banner(soup)['timestamp'],\n",
    "                     link.extract_link(soup),\n",
    "                     link.extract_body_links(soup),\n",
    "                     link.extract_comment_stacked(soup),\n",
    "                     link.extract_comment_item_code(soup)\n",
    "                     ]\n",
    "            # Appends every new profile to a csv file in the provided path\n",
    "            try:\n",
    "                with open(file_path_post, 'a', encoding='utf_8_sig', newline=\"\") as csvfile:\n",
    "                    csvwriter = csv.writer(csvfile)\n",
    "                    csvwriter.writerow(entry)\n",
    "            except:\n",
    "                print('Error while processing data')\n",
    "        \n",
    "        elif item.detect_item_type(i, soup) in ['discussion', 'poll', 'bounty']:\n",
    "            entry = [discussion.extract_title(soup),\n",
    "                     item.detect_item_type(i,soup),\n",
    "                     str(i),\n",
    "                     discussion.extract_banner(soup)['sats'],\n",
    "                     discussion.extract_banner(soup)['boost'],\n",
    "                     discussion.extract_banner(soup)['comments'],\n",
    "                     discussion.extract_banner(soup)['author'],\n",
    "                     discussion.extract_banner(soup)['tag'],\n",
    "                     discussion.extract_banner(soup)['timestamp'],\n",
    "                     None,\n",
    "                     discussion.extract_body_links(soup),\n",
    "                     discussion.extract_comment_stacked(soup),\n",
    "                     discussion.extract_comment_item_code(soup)\n",
    "                     ]\n",
    "            \n",
    "            # Appends every new profile to a csv file in the provided path\n",
    "            try:\n",
    "                with open(file_path_post, 'a', encoding='utf_8_sig', newline=\"\") as csvfile:\n",
    "                    csvwriter = csv.writer(csvfile)\n",
    "                    csvwriter.writerow(entry)\n",
    "            except:\n",
    "                print('Error while processing data')\n",
    "        \n",
    "    except:\n",
    "        continue\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "218bc6335159e0fb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "--------------------------------------------------------------------------------\n",
    "## Saving with SQLite "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6698982d24e26bf9"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Try to scrape 150 different items rather than in 'progressive item mode'\n",
    "from random import sample\n",
    "\n",
    "sampled_items = sample([*range(1,250000)], 100)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T15:14:59.966140558Z",
     "start_time": "2023-10-13T15:14:59.922154281Z"
    }
   },
   "id": "407844d3e7ee49b0"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('../data/stacker_news.sqlite')\n",
    "cur = conn.cursor()\n",
    "\n",
    "sql_comment = \"\"\"\n",
    "DROP TABLE IF EXISTS comments;\n",
    "CREATE TABLE comments (\n",
    "    ItemCode TEXT,\n",
    "    Sats TEXT,\n",
    "    Boost TEXT,\n",
    "    Comments TEXT,\n",
    "    Author TEXT,\n",
    "    Tag TEXT,\n",
    "    Timestamp TEXT,\n",
    "    CommentsItemCode TEXT,\n",
    "    PRIMARY KEY (ItemCode))\n",
    "\"\"\"\n",
    "\n",
    "sql_post = \"\"\"\n",
    "DROP TABLE IF EXISTS post;\n",
    "CREATE TABLE post (\n",
    "    Title TEXT,\n",
    "    Category TEXT,\n",
    "    ItemCode TEXT,\n",
    "    Sats TEXT,\n",
    "    Boost TEXT,\n",
    "    Comments TEXT,\n",
    "    Author TEXT,\n",
    "    Tag TEXT,\n",
    "    Timestamp TEXT,\n",
    "    MainLink TEXT,\n",
    "    BodyLinks TEXT,\n",
    "    SatsReceivedComments TEXT,\n",
    "    CommentsItemCode TEXT,\n",
    "    PRIMARY KEY (ItemCode))\n",
    "\"\"\"\n",
    "\n",
    "sql_exception = \"\"\"\n",
    "DROP TABLE IF EXISTS exceptions;\n",
    "CREATE TABLE exceptions(\n",
    "    RequestResult TEXT,\n",
    "    ItemCode TEXT,\n",
    "    Soup TEXT,\n",
    "    PRIMARY KEY (ItemCode))\n",
    "\"\"\"\n",
    "\n",
    "cur.executescript(sql_comment)\n",
    "cur.executescript(sql_post)\n",
    "cur.executescript(sql_exception)\n",
    "\n",
    "conn.commit()\n",
    "conn.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T15:15:01.406229482Z",
     "start_time": "2023-10-13T15:15:01.370430717Z"
    }
   },
   "id": "70451eecccab5a1c"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "insert_comment = \"\"\"\n",
    "INSERT OR IGNORE INTO comments (\n",
    "    ItemCode,\n",
    "    Sats,\n",
    "    Boost,\n",
    "    Comments,\n",
    "    Author,\n",
    "    Tag,\n",
    "    Timestamp,\n",
    "    CommentsItemCode\n",
    "    ) values (?, ?, ?, ?, ?, ?, ?, ?)\n",
    "\"\"\"\n",
    "\n",
    "insert_post = \"\"\"\n",
    "INSERT OR IGNORE INTO post (\n",
    "    Title,\n",
    "    Category,\n",
    "    ItemCode,\n",
    "    Sats,\n",
    "    Boost,\n",
    "    Comments,\n",
    "    Author,\n",
    "    Tag,\n",
    "    Timestamp,\n",
    "    MainLink,\n",
    "    BodyLinks,\n",
    "    SatsReceivedComments,\n",
    "    CommentsItemCode\n",
    "    ) values (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "\"\"\"\n",
    "\n",
    "insert_exception = \"\"\"\n",
    "INSERT OR IGNORE INTO exceptions (\n",
    "    RequestResult,\n",
    "    ItemCode,\n",
    "    Soup\n",
    "    ) values (?, ?, ?)\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T15:15:02.735336590Z",
     "start_time": "2023-10-13T15:15:02.718101426Z"
    }
   },
   "id": "bd7e16d6db8f28c2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that every entry in the sql statement is transformed into a string, only the `None` values (`NULL` in SQL) are left as is."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "57d4c8de845d26ff"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:43<00:00,  1.03s/it]\n"
     ]
    }
   ],
   "source": [
    "conn = sqlite3.connect('../data/stacker_news.sqlite')\n",
    "cur = conn.cursor()\n",
    "\n",
    "for i in tqdm(sampled_items):\n",
    "    try:\n",
    "        # Provided a string returns a bs4.BeautifulSoup object\n",
    "        url_posts = f'https://stacker.news/items/{i}'\n",
    "        response = requests.get(url_posts)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        if item.detect_item_type(i, soup)=='comment':\n",
    "            # Insert every new entry into a new row in the provided DB\n",
    "            entry = (str(i),\n",
    "                     str(comment.extract_banner(soup)['sats']),\n",
    "                     str(comment.extract_banner(soup)['boost']),\n",
    "                     str(comment.extract_banner(soup)['comments']),\n",
    "                     str(comment.extract_banner(soup)['author']),\n",
    "                     str(comment.extract_banner(soup)['tag']),\n",
    "                     str(comment.extract_banner(soup)['timestamp']),\n",
    "                     str(comment.extract_comment_item_code(soup))\n",
    "                     )\n",
    "            try:\n",
    "                cur.execute(insert_comment, entry)\n",
    "            except:\n",
    "                print(f'Error while inserting the comment item {i} in the database')\n",
    "\n",
    "        elif item.detect_item_type(i, soup)=='link':\n",
    "            # Appends every new profile to a csv file in the provided path\n",
    "            entry = (str(link.extract_title(soup)),\n",
    "                     str(item.detect_item_type(i,soup)),\n",
    "                     str(i),\n",
    "                     str(link.extract_banner(soup)['sats']),\n",
    "                     str(link.extract_banner(soup)['boost']),\n",
    "                     str(link.extract_banner(soup)['comments']),\n",
    "                     str(link.extract_banner(soup)['author']),\n",
    "                     str(link.extract_banner(soup)['tag']),\n",
    "                     str(link.extract_banner(soup)['timestamp']),\n",
    "                     str(link.extract_link(soup)),\n",
    "                     str(link.extract_body_links(soup)),\n",
    "                     str(link.extract_comment_stacked(soup)),\n",
    "                     str(link.extract_comment_item_code(soup))\n",
    "                     )\n",
    "            try:\n",
    "                cur.execute(insert_post, entry)\n",
    "            except:\n",
    "                print(f'Error while inserting the link item {i} in the database')\n",
    "        \n",
    "        elif item.detect_item_type(i, soup) in ['discussion', 'poll', 'bounty']:\n",
    "            entry = (str(discussion.extract_title(soup)),\n",
    "                     str(item.detect_item_type(i,soup)),\n",
    "                     str(i),\n",
    "                     str(discussion.extract_banner(soup)['sats']),\n",
    "                     str(discussion.extract_banner(soup)['boost']),\n",
    "                     str(discussion.extract_banner(soup)['comments']),\n",
    "                     str(discussion.extract_banner(soup)['author']),\n",
    "                     str(discussion.extract_banner(soup)['tag']),\n",
    "                     str(discussion.extract_banner(soup)['timestamp']),\n",
    "                     None,\n",
    "                     str(discussion.extract_body_links(soup)),\n",
    "                     str(discussion.extract_comment_stacked(soup)),\n",
    "                     str(discussion.extract_comment_item_code(soup))\n",
    "                     )\n",
    "            \n",
    "            # Appends every new profile to a csv file in the provided path\n",
    "            try:\n",
    "                cur.execute(insert_post, entry)\n",
    "            except:\n",
    "                print(f'Error while inserting the post item {i} in the database')\n",
    "                \n",
    "        if i%5000==0:\n",
    "            conn.commit()\n",
    "            continue\n",
    "    except:\n",
    "        try:\n",
    "            exception_entry = (\n",
    "                str(response),\n",
    "                str(i),\n",
    "                str(soup)\n",
    "            )\n",
    "            \n",
    "            cur.execute(insert_exception, exception_entry)\n",
    "            \n",
    "        except:\n",
    "            continue\n",
    "\n",
    "conn.commit()\n",
    "cur.close()\n",
    "conn.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T15:16:48.198185194Z",
     "start_time": "2023-10-13T15:15:04.847231680Z"
    }
   },
   "id": "98a83436350bc35c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's try to save SQL query results to a python object in order to further manipulate"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5adb2ad1ddff2795"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Profile scraping\n",
    "\n",
    "**NB**: this code must be run after the end of the whole scraping activity because an `unique(author)` is needed in order to scrape all the user profiles in the forum. \n",
    "\n",
    "**The `unique(author)` must be the result of a `UNION ALL` between the tables.**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "45340082a7ccf465"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('../data/stacker_news.sqlite')\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT DISTINCT Author\n",
    "FROM (\n",
    "    SELECT Author\n",
    "    FROM comments\n",
    "    UNION ALL\n",
    "    SELECT Author\n",
    "    FROM post\n",
    "     );\n",
    "\"\"\"\n",
    "\n",
    "sql_query = pd.read_sql(query, conn)\n",
    "result = pd.DataFrame(sql_query,\n",
    "                      columns=['Author'])\n",
    "\n",
    "conn.close()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cdcc38ce64f686ee"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then this list can be used as an input for the profile scraping script in order to retrieve all the user profiles and insert them into a new `TABLE`."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "93147a4a632d0098"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we need to add a new `table` for the Profiles"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "48b4cbb56b6c8518"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('../data/stacker_news.sqlite')\n",
    "cur = conn.cursor()\n",
    "\n",
    "sql_profile = \"\"\"\n",
    "DROP TABLE IF EXISTS profile;\n",
    "CREATE TABLE profile (\n",
    "    User TEXT,\n",
    "    TotalStacked TEXT,\n",
    "    FirstItem TEXT,\n",
    "    HatStreak TEXT,\n",
    "    NumItems TEXT,\n",
    "    PRIMARY KEY (User))\n",
    "\"\"\"\n",
    "\n",
    "cur.executescript(sql_profile)\n",
    "\n",
    "conn.commit()\n",
    "conn.close()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "df16eb4fc5e6b153"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "insert_profile = \"\"\"\n",
    "INSERT INTO post (\n",
    "    User,\n",
    "    TotalStacked,\n",
    "    FirstItem,\n",
    "    HatStreak,\n",
    "    NumItems\n",
    "    ) values (?, ?, ?, ?, ?)\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ba0d83ffa888f3fe"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "user.get_profile(\"DarthCoin\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5c71b7e298f50189"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# cur = conn.cursor()\n",
    "# \n",
    "# for i in user_list:\n",
    "#     try:\n",
    "#         profile_data = user.get_profile(i)\n",
    "#         \n",
    "#     except:\n",
    "#         pass\n",
    "# \n",
    "# conn.commit()\n",
    "# conn.close()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c9be74d662451a6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
