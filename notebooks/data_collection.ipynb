{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Data collection activities\n",
    "All the data collection activities are automated using user defined functions retrievable in the folder `scripts`."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8cef29dc85632b19"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests as requests\n",
    "from scripts import item \n",
    "from scripts import discussion \n",
    "from scripts import link\n",
    "from scripts import comment\n",
    "from scripts import user\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import sqlite3\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-27T13:29:30.275171542Z",
     "start_time": "2023-10-27T13:29:29.593334202Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "url_posts = f'https://stacker.news/items/21'\n",
    "response = requests.get(url_posts)\n",
    "response.raise_for_status()\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "banner = soup.find('div', class_='item_other__MjgP3')\n",
    "\n",
    "partial_banner_data = [i.text for i in banner.find_all('span')]\n",
    "\n",
    "print(partial_banner_data)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b776c4b6eddd322"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Item scraping\n",
    "The following code saves items data into a csv file, provided a range of item codes fixed by the operator.\n",
    "\n",
    "First of all we need to initialize all the files for data collection"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b87be68b26ebf12"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Initialization of csv files"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c03ba9743017eaa9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Post items\n",
    "file_path_post = \"../data/post.csv\"\n",
    "row_head_post = [\"Title\",\n",
    "                 \"Category\",\n",
    "                 \"Item code\",\n",
    "                 \"Sats\",\n",
    "                 \"Boost\",\n",
    "                 \"Comments\",\n",
    "                 \"Author\",\n",
    "                 \"Tag\",\n",
    "                 \"Timestamp\",\n",
    "                 \"Main link\",\n",
    "                 \"Body links\",\n",
    "                 \"Sats received by comments\",\n",
    "                 \"Comments item code\",\n",
    "                 ]\n",
    "    \n",
    "with open(file_path_post, 'w', encoding='utf_8_sig', newline=\"\") as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerow(row_head_post)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6564de73a2be1f53"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Comment items\n",
    "file_path_comment = \"../data/comment.csv\"\n",
    "row_head_comment = [\"Item code\",\n",
    "                    \"Sats\",\n",
    "                    \"Boost\",\n",
    "                    \"Comments\",\n",
    "                    \"Author\",\n",
    "                    \"Tag\",\n",
    "                    \"Timestamp\",\n",
    "                    \"Comments item code\",\n",
    "                    ]\n",
    "    \n",
    "with open(file_path_comment, 'w', encoding='utf_8_sig', newline=\"\") as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerow(row_head_comment)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ebcd59394d6e5494"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Try to scrape 150 different items rather than in 'progressive item mode'\n",
    "from random import sample\n",
    "\n",
    "sampled_items = sample([*range(1,200000)], 10)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e819de3d57c3aa0a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(sampled_items)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5622bfb2d27b3ab8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Csv and data structure\n",
    "\n",
    "### Columns to be used for all the post item scraping\n",
    "\n",
    "- Title\n",
    "- Category\n",
    "- Item code\n",
    "- Banner data\n",
    "- Main link\n",
    "- Body links\n",
    "- Sats received by comments\n",
    "- Comment item code\n",
    "\n",
    "### Columns to be used for the comment item scraping\n",
    "- Item code\n",
    "- Banner data\n",
    "- Comment item code\n",
    "\n",
    "**NB**-> the `comment item code` in Comment item table could even be deleted, we can just keep it in order to eventually see the relationship between the comment and the comments to the specific comment"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "342b0be052ee1177"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Optimizing for the similarities between item post types\n",
    "\n",
    "In the following chunck the functions applied are clustered for similarities, leading to three blocks:\n",
    "- Comment\n",
    "- Link\n",
    "- Discussion/poll/bounty\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "37b9b2089d27f466"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i in tqdm(sampled_items):\n",
    "    try:\n",
    "        # Provided a string returns a bs4.BeautifulSoup object\n",
    "        url_posts = f'https://stacker.news/items/{i}'\n",
    "        response = requests.get(url_posts)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        if item.detect_item_type(i, soup)=='comment':\n",
    "            entry = [str(i),\n",
    "                     comment.extract_banner(soup)['sats'],\n",
    "                     comment.extract_banner(soup)['boost'],\n",
    "                     comment.extract_banner(soup)['comments'],\n",
    "                     comment.extract_banner(soup)['author'],\n",
    "                     comment.extract_banner(soup)['tag'],\n",
    "                     comment.extract_banner(soup)['timestamp'],\n",
    "                     comment.extract_comment_item_code(soup)\n",
    "                     ]\n",
    "            \n",
    "            # Appends every new profile to a csv file in the provided path\n",
    "            try:\n",
    "                with open(file_path_comment, 'a', encoding='utf_8_sig', newline=\"\") as csvfile:\n",
    "                    csvwriter = csv.writer(csvfile)\n",
    "                    csvwriter.writerow(entry)\n",
    "            except:\n",
    "                print('Error while processing data')\n",
    "\n",
    "        elif item.detect_item_type(i, soup)=='link':\n",
    "            entry = [link.extract_title(soup),\n",
    "                     item.detect_item_type(i,soup),\n",
    "                     str(i),\n",
    "                     link.extract_banner(soup)['sats'],\n",
    "                     link.extract_banner(soup)['boost'],\n",
    "                     link.extract_banner(soup)['comments'],\n",
    "                     link.extract_banner(soup)['author'],\n",
    "                     link.extract_banner(soup)['tag'],\n",
    "                     link.extract_banner(soup)['timestamp'],\n",
    "                     link.extract_link(soup),\n",
    "                     link.extract_body_links(soup),\n",
    "                     link.extract_comment_stacked(soup),\n",
    "                     link.extract_comment_item_code(soup)\n",
    "                     ]\n",
    "            # Appends every new profile to a csv file in the provided path\n",
    "            try:\n",
    "                with open(file_path_post, 'a', encoding='utf_8_sig', newline=\"\") as csvfile:\n",
    "                    csvwriter = csv.writer(csvfile)\n",
    "                    csvwriter.writerow(entry)\n",
    "            except:\n",
    "                print('Error while processing data')\n",
    "        \n",
    "        elif item.detect_item_type(i, soup) in ['discussion', 'poll', 'bounty']:\n",
    "            entry = [discussion.extract_title(soup),\n",
    "                     item.detect_item_type(i,soup),\n",
    "                     str(i),\n",
    "                     discussion.extract_banner(soup)['sats'],\n",
    "                     discussion.extract_banner(soup)['boost'],\n",
    "                     discussion.extract_banner(soup)['comments'],\n",
    "                     discussion.extract_banner(soup)['author'],\n",
    "                     discussion.extract_banner(soup)['tag'],\n",
    "                     discussion.extract_banner(soup)['timestamp'],\n",
    "                     None,\n",
    "                     discussion.extract_body_links(soup),\n",
    "                     discussion.extract_comment_stacked(soup),\n",
    "                     discussion.extract_comment_item_code(soup)\n",
    "                     ]\n",
    "            \n",
    "            # Appends every new profile to a csv file in the provided path\n",
    "            try:\n",
    "                with open(file_path_post, 'a', encoding='utf_8_sig', newline=\"\") as csvfile:\n",
    "                    csvwriter = csv.writer(csvfile)\n",
    "                    csvwriter.writerow(entry)\n",
    "            except:\n",
    "                print('Error while processing data')\n",
    "        \n",
    "    except:\n",
    "        continue\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "218bc6335159e0fb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "--------------------------------------------------------------------------------\n",
    "## Saving with SQLite "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6698982d24e26bf9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Try to scrape 150 different items rather than in 'progressive item mode'\n",
    "from random import sample\n",
    "\n",
    "sampled_items = sample([*range(1,250000)], 100)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "407844d3e7ee49b0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('../data/stacker_news.sqlite')\n",
    "cur = conn.cursor()\n",
    "\n",
    "sql_comment = \"\"\"\n",
    "DROP TABLE IF EXISTS comments;\n",
    "CREATE TABLE comments (\n",
    "    ItemCode TEXT,\n",
    "    Sats TEXT,\n",
    "    Boost TEXT,\n",
    "    Comments TEXT,\n",
    "    Author TEXT,\n",
    "    Tag TEXT,\n",
    "    Timestamp TEXT,\n",
    "    CommentsItemCode TEXT,\n",
    "    PRIMARY KEY (ItemCode))\n",
    "\"\"\"\n",
    "\n",
    "sql_post = \"\"\"\n",
    "DROP TABLE IF EXISTS post;\n",
    "CREATE TABLE post (\n",
    "    Title TEXT,\n",
    "    Category TEXT,\n",
    "    ItemCode TEXT,\n",
    "    Sats TEXT,\n",
    "    Boost TEXT,\n",
    "    Comments TEXT,\n",
    "    Author TEXT,\n",
    "    Tag TEXT,\n",
    "    Timestamp TEXT,\n",
    "    MainLink TEXT,\n",
    "    BodyLinks TEXT,\n",
    "    SatsReceivedComments TEXT,\n",
    "    CommentsItemCode TEXT,\n",
    "    PRIMARY KEY (ItemCode))\n",
    "\"\"\"\n",
    "\n",
    "sql_exception = \"\"\"\n",
    "DROP TABLE IF EXISTS exceptions;\n",
    "CREATE TABLE exceptions(\n",
    "    RequestResult TEXT,\n",
    "    ItemCode TEXT,\n",
    "    Soup TEXT,\n",
    "    PRIMARY KEY (ItemCode))\n",
    "\"\"\"\n",
    "\n",
    "cur.executescript(sql_comment)\n",
    "cur.executescript(sql_post)\n",
    "cur.executescript(sql_exception)\n",
    "\n",
    "conn.commit()\n",
    "conn.close()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "70451eecccab5a1c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "insert_comment = \"\"\"\n",
    "INSERT OR IGNORE INTO comments (\n",
    "    ItemCode,\n",
    "    Sats,\n",
    "    Boost,\n",
    "    Comments,\n",
    "    Author,\n",
    "    Tag,\n",
    "    Timestamp,\n",
    "    CommentsItemCode\n",
    "    ) values (?, ?, ?, ?, ?, ?, ?, ?)\n",
    "\"\"\"\n",
    "\n",
    "insert_post = \"\"\"\n",
    "INSERT OR IGNORE INTO post (\n",
    "    Title,\n",
    "    Category,\n",
    "    ItemCode,\n",
    "    Sats,\n",
    "    Boost,\n",
    "    Comments,\n",
    "    Author,\n",
    "    Tag,\n",
    "    Timestamp,\n",
    "    MainLink,\n",
    "    BodyLinks,\n",
    "    SatsReceivedComments,\n",
    "    CommentsItemCode\n",
    "    ) values (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "\"\"\"\n",
    "\n",
    "insert_exception = \"\"\"\n",
    "INSERT OR IGNORE INTO exceptions (\n",
    "    RequestResult,\n",
    "    ItemCode,\n",
    "    Soup\n",
    "    ) values (?, ?, ?)\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bd7e16d6db8f28c2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that every entry in the sql statement is transformed into a string, only the `None` values (`NULL` in SQL) are left as is."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "57d4c8de845d26ff"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('../data/stacker_news.sqlite')\n",
    "cur = conn.cursor()\n",
    "\n",
    "for i in tqdm(sampled_items):\n",
    "    try:\n",
    "        # Provided a string returns a bs4.BeautifulSoup object\n",
    "        url_posts = f'https://stacker.news/items/{i}'\n",
    "        response = requests.get(url_posts)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        if item.detect_item_type(i, soup)=='comment':\n",
    "            # Insert every new entry into a new row in the provided DB\n",
    "            entry = (str(i),\n",
    "                     str(comment.extract_banner(soup)['sats']),\n",
    "                     str(comment.extract_banner(soup)['boost']),\n",
    "                     str(comment.extract_banner(soup)['comments']),\n",
    "                     str(comment.extract_banner(soup)['author']),\n",
    "                     str(comment.extract_banner(soup)['tag']),\n",
    "                     str(comment.extract_banner(soup)['timestamp']),\n",
    "                     str(comment.extract_comment_item_code(soup))\n",
    "                     )\n",
    "            try:\n",
    "                cur.execute(insert_comment, entry)\n",
    "            except:\n",
    "                print(f'Error while inserting the comment item {i} in the database')\n",
    "\n",
    "        elif item.detect_item_type(i, soup)=='link':\n",
    "            # Appends every new profile to a csv file in the provided path\n",
    "            entry = (str(link.extract_title(soup)),\n",
    "                     str(item.detect_item_type(i,soup)),\n",
    "                     str(i),\n",
    "                     str(link.extract_banner(soup)['sats']),\n",
    "                     str(link.extract_banner(soup)['boost']),\n",
    "                     str(link.extract_banner(soup)['comments']),\n",
    "                     str(link.extract_banner(soup)['author']),\n",
    "                     str(link.extract_banner(soup)['tag']),\n",
    "                     str(link.extract_banner(soup)['timestamp']),\n",
    "                     str(link.extract_link(soup)),\n",
    "                     str(link.extract_body_links(soup)),\n",
    "                     str(link.extract_comment_stacked(soup)),\n",
    "                     str(link.extract_comment_item_code(soup))\n",
    "                     )\n",
    "            try:\n",
    "                cur.execute(insert_post, entry)\n",
    "            except:\n",
    "                print(f'Error while inserting the link item {i} in the database')\n",
    "        \n",
    "        elif item.detect_item_type(i, soup) in ['discussion', 'poll', 'bounty']:\n",
    "            entry = (str(discussion.extract_title(soup)),\n",
    "                     str(item.detect_item_type(i,soup)),\n",
    "                     str(i),\n",
    "                     str(discussion.extract_banner(soup)['sats']),\n",
    "                     str(discussion.extract_banner(soup)['boost']),\n",
    "                     str(discussion.extract_banner(soup)['comments']),\n",
    "                     str(discussion.extract_banner(soup)['author']),\n",
    "                     str(discussion.extract_banner(soup)['tag']),\n",
    "                     str(discussion.extract_banner(soup)['timestamp']),\n",
    "                     None,\n",
    "                     str(discussion.extract_body_links(soup)),\n",
    "                     str(discussion.extract_comment_stacked(soup)),\n",
    "                     str(discussion.extract_comment_item_code(soup))\n",
    "                     )\n",
    "            \n",
    "            # Appends every new profile to a csv file in the provided path\n",
    "            try:\n",
    "                cur.execute(insert_post, entry)\n",
    "            except:\n",
    "                print(f'Error while inserting the post item {i} in the database')\n",
    "                \n",
    "        if i%5000==0:\n",
    "            conn.commit()\n",
    "            continue\n",
    "    except:\n",
    "        try:\n",
    "            exception_entry = (\n",
    "                str(response),\n",
    "                str(i),\n",
    "                str(soup)\n",
    "            )\n",
    "            \n",
    "            cur.execute(insert_exception, exception_entry)\n",
    "            \n",
    "        except:\n",
    "            continue\n",
    "\n",
    "conn.commit()\n",
    "cur.close()\n",
    "conn.close()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "98a83436350bc35c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's try to save SQL query results to a python object in order to further manipulate"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5adb2ad1ddff2795"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Profile scraping\n",
    "\n",
    "**NB**: this code must be run after the end of the whole scraping activity because an `unique(author)` is needed in order to scrape all the user profiles in the forum. \n",
    "\n",
    "**The `unique(author)` must be the result of a `UNION ALL` between the tables.**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "45340082a7ccf465"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('../data/stacker_news.sqlite')\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT DISTINCT Author\n",
    "FROM (\n",
    "    SELECT Author\n",
    "    FROM comments\n",
    "    UNION ALL\n",
    "    SELECT Author\n",
    "    FROM post\n",
    "     );\n",
    "\"\"\"\n",
    "\n",
    "sql_query = pd.read_sql(query, conn)\n",
    "result = pd.DataFrame(sql_query,\n",
    "                      columns=['Author'])\n",
    "\n",
    "conn.close()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-27T13:29:35.972313681Z",
     "start_time": "2023-10-27T13:29:35.738280450Z"
    }
   },
   "id": "cdcc38ce64f686ee"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then this list can be used as an input for the profile scraping script in order to retrieve all the user profiles and insert them into a new `TABLE`."
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-27T10:52:50.547695112Z",
     "start_time": "2023-10-27T10:52:50.506001208Z"
    }
   },
   "id": "93147a4a632d0098"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we need to add a new `table` for the Profiles"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "48b4cbb56b6c8518"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('../data/stacker_news.sqlite')\n",
    "cur = conn.cursor()\n",
    "\n",
    "sql_user = \"\"\"\n",
    "DROP TABLE IF EXISTS user;\n",
    "CREATE TABLE user (\n",
    "    User TEXT,\n",
    "    TotalStacked TEXT,\n",
    "    FirstItem TEXT,\n",
    "    HatStreak TEXT,\n",
    "    NumItems TEXT,\n",
    "    PRIMARY KEY (User))\n",
    "\"\"\"\n",
    "\n",
    "cur.executescript(sql_user)\n",
    "\n",
    "conn.commit()\n",
    "conn.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-27T13:29:37.863934088Z",
     "start_time": "2023-10-27T13:29:37.843733625Z"
    }
   },
   "id": "df16eb4fc5e6b153"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "insert_user = \"\"\"\n",
    "INSERT INTO user (\n",
    "    User,\n",
    "    TotalStacked,\n",
    "    FirstItem,\n",
    "    HatStreak,\n",
    "    NumItems\n",
    "    ) values (?, ?, ?, ?, ?)\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-27T13:29:39.282311816Z",
     "start_time": "2023-10-27T13:29:39.257659592Z"
    }
   },
   "id": "ba0d83ffa888f3fe"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "for u in result['Author'][1:10]:\n",
    "    user.get_profile(u)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-27T13:29:46.995508782Z",
     "start_time": "2023-10-27T13:29:40.410387854Z"
    }
   },
   "id": "5c71b7e298f50189"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5806/5806 [1:02:07<00:00,  1.56it/s]\n"
     ]
    }
   ],
   "source": [
    "conn = sqlite3.connect('../data/stacker_news.sqlite')\n",
    "cur = conn.cursor()\n",
    "\n",
    "for i in tqdm(result['Author']):\n",
    "    try:\n",
    "        profile_data = user.get_profile(i)\n",
    "        entry = (\n",
    "            str(profile_data[0]),\n",
    "            str(profile_data[1]),\n",
    "            str(profile_data[2]),\n",
    "            str(profile_data[3]),\n",
    "            str(profile_data[4])\n",
    "        )\n",
    "        try:\n",
    "            cur.execute(insert_user, entry)\n",
    "        except:\n",
    "            continue\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "conn.commit()\n",
    "cur.close()\n",
    "conn.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-27T14:31:58.435291068Z",
     "start_time": "2023-10-27T13:29:50.581719671Z"
    }
   },
   "id": "c9be74d662451a6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
