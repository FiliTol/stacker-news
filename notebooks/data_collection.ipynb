{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Data collection activities\n",
    "All the data collection activities are automated using user defined functions retrievable in the folder `scripts`."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8cef29dc85632b19"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-11T20:49:38.545712007Z",
     "start_time": "2023-10-11T20:49:38.222526729Z"
    }
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests as requests\n",
    "from scripts import user, item, discussion, link, poll, bounty, comment\n",
    "import csv\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Item scraping\n",
    "The following code saves items data into a csv file, provided a range of item codes fixed by the operator.\n",
    "\n",
    "First of all we need to initialize all the files for data collection"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b87be68b26ebf12"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Initialization of csv files"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c03ba9743017eaa9"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Post items\n",
    "file_path_post = \"../data/post.csv\"\n",
    "row_head_post = [\"Title\",\n",
    "                 \"Category\",\n",
    "                 \"Item code\",\n",
    "                 \"Banner data\",\n",
    "                 \"Main link\",\n",
    "                 \"Body links\",\n",
    "                 \"Sats received by comments\",\n",
    "                 \"Comments item code\",\n",
    "                 ]\n",
    "    \n",
    "with open(file_path_post, 'w', encoding='utf_8_sig', newline=\"\") as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerow(row_head_post)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T20:49:40.183262054Z",
     "start_time": "2023-10-11T20:49:40.175353406Z"
    }
   },
   "id": "6564de73a2be1f53"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Comment items\n",
    "file_path_comment = \"../data/comment.csv\"\n",
    "row_head_comment = [\"Item code\",\n",
    "                    \"Banner data\",\n",
    "                    \"Comments item code\",\n",
    "                    ]\n",
    "    \n",
    "with open(file_path_comment, 'w', encoding='utf_8_sig', newline=\"\") as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerow(row_head_comment)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T20:49:41.081444443Z",
     "start_time": "2023-10-11T20:49:41.074190907Z"
    }
   },
   "id": "ebcd59394d6e5494"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Try to scrape 150 different items rather than in 'progressive item mode'\n",
    "from random import sample\n",
    "\n",
    "sampled_items = sample([*range(1,200000)], 10)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T20:49:41.875900815Z",
     "start_time": "2023-10-11T20:49:41.872030325Z"
    }
   },
   "id": "e819de3d57c3aa0a"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[151041, 25864, 166960, 106144, 32060, 64643, 62542, 63703, 31873, 70302]\n"
     ]
    }
   ],
   "source": [
    "print(sampled_items)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T20:49:42.939727882Z",
     "start_time": "2023-10-11T20:49:42.931887059Z"
    }
   },
   "id": "5622bfb2d27b3ab8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## First attempt"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c287810e4c776ebd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# for i in tqdm(sampled_items):\n",
    "#     try:\n",
    "#         # Provided a string returns a bs4.BeautifulSoup object\n",
    "#         url_posts = f'https://stacker.news/items/{i}'\n",
    "#         response = requests.get(url_posts)\n",
    "#         response.raise_for_status()\n",
    "#         soup = BeautifulSoup(response.text, 'html.parser')\n",
    "#         \n",
    "#         if item.detect_item_type(i, soup)=='comment':\n",
    "#             entry = [None,\n",
    "#                      str(i),\n",
    "#                      comment.extract_banner(soup),\n",
    "#                      comment.extract_body_links(soup),\n",
    "#                      comment.extract_comment_stacked(soup),\n",
    "#                      comment.extract_comment_item_code(soup)\n",
    "#                      ]\n",
    "#             \n",
    "#             # Appends every new profile to a csv file in the provided path\n",
    "#             try:\n",
    "#                 with open(file_path_comment, 'a', encoding='utf_8_sig', newline=\"\") as csvfile:\n",
    "#                     csvwriter = csv.writer(csvfile)\n",
    "#                     csvwriter.writerow(entry)\n",
    "#             except:\n",
    "#                 print('Error while processing data')\n",
    "#         \n",
    "#         elif item.detect_item_type(i, soup)=='discussion':\n",
    "#             entry = [discussion.extract_title(soup),\n",
    "#                      str(i),\n",
    "#                      discussion.extract_banner(soup),\n",
    "#                      discussion.extract_body_links(soup),\n",
    "#                      discussion.extract_comment_stacked(soup),\n",
    "#                      discussion.extract_comment_item_code(soup)\n",
    "#                      ]\n",
    "#             \n",
    "#             # Appends every new profile to a csv file in the provided path\n",
    "#             try:\n",
    "#                 with open(file_path_discussion, 'a', encoding='utf_8_sig', newline=\"\") as csvfile:\n",
    "#                     csvwriter = csv.writer(csvfile)\n",
    "#                     csvwriter.writerow(entry)\n",
    "#             except:\n",
    "#                 print('Error while processing data')\n",
    "#         \n",
    "#         elif item.detect_item_type(i, soup)=='link':\n",
    "#             entry = [link.extract_title(soup),\n",
    "#                      str(i),\n",
    "#                      link.extract_banner(soup),\n",
    "#                      link.extract_link(soup),\n",
    "#                      link.extract_body_links(soup),\n",
    "#                      link.extract_comment_stacked(soup),\n",
    "#                      link.extract_comment_item_code(soup)\n",
    "#                      ]\n",
    "#             # Appends every new profile to a csv file in the provided path\n",
    "#             try:\n",
    "#                 with open(file_path_link, 'a', encoding='utf_8_sig', newline=\"\") as csvfile:\n",
    "#                     csvwriter = csv.writer(csvfile)\n",
    "#                     csvwriter.writerow(entry)\n",
    "#             except:\n",
    "#                 print('Error while processing data')\n",
    "#                 \n",
    "#         elif item.detect_item_type(i, soup)=='poll':\n",
    "#             entry = [poll.extract_title(soup),\n",
    "#                      str(i),\n",
    "#                      poll.extract_banner(soup),\n",
    "#                      poll.extract_body_links(soup),\n",
    "#                      poll.extract_comment_stacked(soup),\n",
    "#                      poll.extract_comment_item_code(soup)\n",
    "#                      ]\n",
    "#             # Appends every new profile to a csv file in the provided path\n",
    "#             try:\n",
    "#                 with open(file_path_poll, 'a', encoding='utf_8_sig', newline=\"\") as csvfile:\n",
    "#                     csvwriter = csv.writer(csvfile)\n",
    "#                     csvwriter.writerow(entry)\n",
    "#             except:\n",
    "#                 print('Error while processing data')\n",
    "#         \n",
    "#         elif item.detect_item_type(i, soup)=='bounty':\n",
    "#             entry = [bounty.extract_title(soup),\n",
    "#                      str(i),\n",
    "#                      bounty.extract_banner(soup),\n",
    "#                      bounty.extract_body_links(soup),\n",
    "#                      bounty.extract_comment_stacked(soup),\n",
    "#                      bounty.extract_comment_item_code(soup)\n",
    "#                      ]\n",
    "#             # Appends every new profile to a csv file in the provided path\n",
    "#             try:\n",
    "#                 with open(file_path_bounty, 'a', encoding='utf_8_sig', newline=\"\") as csvfile:\n",
    "#                     csvwriter = csv.writer(csvfile)\n",
    "#                     csvwriter.writerow(entry)\n",
    "#             except:\n",
    "#                 print('Error while processing data')\n",
    "#         \n",
    "#     except:\n",
    "#         continue\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4f0d197720b15859"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fixing the resulting csv and the data structure\n",
    "\n",
    "### Columns to be used for all the post item scraping\n",
    "\n",
    "- Title\n",
    "- Category\n",
    "- Item code\n",
    "- Banner data\n",
    "- Main link\n",
    "- Body links\n",
    "- Sats received by comments\n",
    "- Comment item code\n",
    "\n",
    "### Columns to be used for the comment item scraping\n",
    "- Item code\n",
    "- Banner data\n",
    "- Comment item code\n",
    "\n",
    "**NB**-> the `comment item code` in Comment item table could even be deleted, we can just keep it in order to eventually see the relationship between the comment and the comments to the specific comment"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "342b0be052ee1177"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# for i in tqdm(sampled_items):\n",
    "#     try:\n",
    "#         # Provided a string returns a bs4.BeautifulSoup object\n",
    "#         url_posts = f'https://stacker.news/items/{i}'\n",
    "#         response = requests.get(url_posts)\n",
    "#         response.raise_for_status()\n",
    "#         soup = BeautifulSoup(response.text, 'html.parser')\n",
    "#         \n",
    "#         if item.detect_item_type(i, soup)=='comment':\n",
    "#             entry = [str(i),\n",
    "#                      comment.extract_banner(soup),\n",
    "#                      comment.extract_comment_item_code(soup)\n",
    "#                      ]\n",
    "#             \n",
    "#             # Appends every new profile to a csv file in the provided path\n",
    "#             try:\n",
    "#                 with open(file_path_comment, 'a', encoding='utf_8_sig', newline=\"\") as csvfile:\n",
    "#                     csvwriter = csv.writer(csvfile)\n",
    "#                     csvwriter.writerow(entry)\n",
    "#             except:\n",
    "#                 print('Error while processing data')\n",
    "# \n",
    "#         elif item.detect_item_type(i, soup)=='link':\n",
    "#             entry = [link.extract_title(soup),\n",
    "#                      item.detect_item_type(i,soup),\n",
    "#                      str(i),\n",
    "#                      link.extract_banner(soup),\n",
    "#                      link.extract_link(soup),\n",
    "#                      link.extract_body_links(soup),\n",
    "#                      link.extract_comment_stacked(soup),\n",
    "#                      link.extract_comment_item_code(soup)\n",
    "#                      ]\n",
    "#             # Appends every new profile to a csv file in the provided path\n",
    "#             try:\n",
    "#                 with open(file_path_post, 'a', encoding='utf_8_sig', newline=\"\") as csvfile:\n",
    "#                     csvwriter = csv.writer(csvfile)\n",
    "#                     csvwriter.writerow(entry)\n",
    "#             except:\n",
    "#                 print('Error while processing data')\n",
    "#         \n",
    "#         elif item.detect_item_type(i, soup)=='discussion':\n",
    "#             entry = [discussion.extract_title(soup),\n",
    "#                      item.detect_item_type(i,soup),\n",
    "#                      str(i),\n",
    "#                      discussion.extract_banner(soup),\n",
    "#                      None,\n",
    "#                      discussion.extract_body_links(soup),\n",
    "#                      discussion.extract_comment_stacked(soup),\n",
    "#                      discussion.extract_comment_item_code(soup)\n",
    "#                      ]\n",
    "#             \n",
    "#             # Appends every new profile to a csv file in the provided path\n",
    "#             try:\n",
    "#                 with open(file_path_post, 'a', encoding='utf_8_sig', newline=\"\") as csvfile:\n",
    "#                     csvwriter = csv.writer(csvfile)\n",
    "#                     csvwriter.writerow(entry)\n",
    "#             except:\n",
    "#                 print('Error while processing data')\n",
    "#                         \n",
    "#         elif item.detect_item_type(i, soup)=='poll':\n",
    "#             entry = [poll.extract_title(soup),\n",
    "#                      item.detect_item_type(i, soup),\n",
    "#                      str(i),\n",
    "#                      poll.extract_banner(soup),\n",
    "#                      None,\n",
    "#                      poll.extract_body_links(soup),\n",
    "#                      poll.extract_comment_stacked(soup),\n",
    "#                      poll.extract_comment_item_code(soup)\n",
    "#                      ]\n",
    "#             # Appends every new profile to a csv file in the provided path\n",
    "#             try:\n",
    "#                 with open(file_path_post, 'a', encoding='utf_8_sig', newline=\"\") as csvfile:\n",
    "#                     csvwriter = csv.writer(csvfile)\n",
    "#                     csvwriter.writerow(entry)\n",
    "#             except:\n",
    "#                 print('Error while processing data')\n",
    "#         \n",
    "#         elif item.detect_item_type(i, soup)=='bounty':\n",
    "#             entry = [bounty.extract_title(soup),\n",
    "#                      item.detect_item_type(i, soup),\n",
    "#                      str(i),\n",
    "#                      bounty.extract_banner(soup),\n",
    "#                      None,\n",
    "#                      bounty.extract_body_links(soup),\n",
    "#                      bounty.extract_comment_stacked(soup),\n",
    "#                      bounty.extract_comment_item_code(soup)\n",
    "#                      ]\n",
    "#             # Appends every new profile to a csv file in the provided path\n",
    "#             try:\n",
    "#                 with open(file_path_post, 'a', encoding='utf_8_sig', newline=\"\") as csvfile:\n",
    "#                     csvwriter = csv.writer(csvfile)\n",
    "#                     csvwriter.writerow(entry)\n",
    "#             except:\n",
    "#                 print('Error while processing data')\n",
    "#         \n",
    "#     except:\n",
    "#         continue\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2d4a7393bff6ea1a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Optimizing for the similarities between item post types\n",
    "\n",
    "In the following chunck the functions applied are clustered for similarities, leading to three blocks:\n",
    "- Comment\n",
    "- Link\n",
    "- Discussion/poll/bounty\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "37b9b2089d27f466"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:35<00:00,  3.52s/it]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(sampled_items):\n",
    "    try:\n",
    "        # Provided a string returns a bs4.BeautifulSoup object\n",
    "        url_posts = f'https://stacker.news/items/{i}'\n",
    "        response = requests.get(url_posts)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        if item.detect_item_type(i, soup)=='comment':\n",
    "            entry = [str(i),\n",
    "                     comment.extract_banner(soup),\n",
    "                     comment.extract_comment_item_code(soup)\n",
    "                     ]\n",
    "            \n",
    "            # Appends every new profile to a csv file in the provided path\n",
    "            try:\n",
    "                with open(file_path_comment, 'a', encoding='utf_8_sig', newline=\"\") as csvfile:\n",
    "                    csvwriter = csv.writer(csvfile)\n",
    "                    csvwriter.writerow(entry)\n",
    "            except:\n",
    "                print('Error while processing data')\n",
    "\n",
    "        elif item.detect_item_type(i, soup)=='link':\n",
    "            entry = [link.extract_title(soup),\n",
    "                     item.detect_item_type(i,soup),\n",
    "                     str(i),\n",
    "                     link.extract_banner(soup),\n",
    "                     link.extract_link(soup),\n",
    "                     link.extract_body_links(soup),\n",
    "                     link.extract_comment_stacked(soup),\n",
    "                     link.extract_comment_item_code(soup)\n",
    "                     ]\n",
    "            # Appends every new profile to a csv file in the provided path\n",
    "            try:\n",
    "                with open(file_path_post, 'a', encoding='utf_8_sig', newline=\"\") as csvfile:\n",
    "                    csvwriter = csv.writer(csvfile)\n",
    "                    csvwriter.writerow(entry)\n",
    "            except:\n",
    "                print('Error while processing data')\n",
    "        \n",
    "        elif item.detect_item_type(i, soup) in ['discussion', 'poll', 'bounty']:\n",
    "            entry = [discussion.extract_title(soup),\n",
    "                     item.detect_item_type(i,soup),\n",
    "                     str(i),\n",
    "                     discussion.extract_banner(soup),\n",
    "                     None,\n",
    "                     discussion.extract_body_links(soup),\n",
    "                     discussion.extract_comment_stacked(soup),\n",
    "                     discussion.extract_comment_item_code(soup)\n",
    "                     ]\n",
    "            \n",
    "            # Appends every new profile to a csv file in the provided path\n",
    "            try:\n",
    "                with open(file_path_post, 'a', encoding='utf_8_sig', newline=\"\") as csvfile:\n",
    "                    csvwriter = csv.writer(csvfile)\n",
    "                    csvwriter.writerow(entry)\n",
    "            except:\n",
    "                print('Error while processing data')\n",
    "        \n",
    "    except:\n",
    "        continue\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T20:50:37.622180512Z",
     "start_time": "2023-10-11T20:50:02.424131671Z"
    }
   },
   "id": "218bc6335159e0fb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
