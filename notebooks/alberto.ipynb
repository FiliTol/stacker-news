{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests as requests\n",
    "import pandas as pd\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-09T12:48:39.526910853Z",
     "start_time": "2023-10-09T12:48:38.991609638Z"
    }
   },
   "id": "49cdbd18047bcedd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-10-09T12:48:41.417576734Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing 1\n",
      "Doing 2\n",
      "Doing 3\n",
      "Doing 4\n",
      "Doing 5\n",
      "Doing 6\n",
      "Doing 7\n",
      "Doing 8\n",
      "Doing 9\n",
      "Doing 10\n",
      "Doing 11\n",
      "Doing 12\n",
      "Doing 13\n",
      "Doing 14\n",
      "Doing 15\n",
      "Doing 16\n",
      "Doing 17\n",
      "Doing 18\n",
      "Doing 19\n",
      "Doing 20\n",
      "Doing 21\n",
      "Doing 22\n",
      "Doing 23\n",
      "Doing 24\n",
      "Doing 25\n",
      "Doing 26\n",
      "Doing 27\n",
      "Doing 28\n",
      "Doing 29\n",
      "Doing 30\n",
      "Doing 31\n",
      "Doing 32\n",
      "Doing 33\n",
      "Doing 34\n",
      "Doing 35\n",
      "Doing 36\n",
      "Doing 37\n",
      "Doing 38\n",
      "Doing 39\n",
      "Doing 40\n",
      "Doing 41\n",
      "Doing 42\n",
      "Doing 43\n",
      "Doing 44\n",
      "Doing 45\n",
      "Doing 46\n",
      "Doing 47\n",
      "Doing 48\n",
      "Doing 49\n",
      "Doing 50\n",
      "Doing 51\n",
      "Doing 52\n",
      "Doing 53\n",
      "Doing 54\n",
      "Doing 55\n",
      "Doing 56\n",
      "Doing 57\n",
      "Doing 58\n",
      "Doing 59\n",
      "Doing 60\n",
      "Doing 61\n",
      "Doing 62\n",
      "Doing 63\n",
      "Doing 64\n",
      "Doing 65\n",
      "Doing 66\n",
      "Doing 67\n",
      "Doing 68\n",
      "Doing 69\n",
      "Doing 70\n",
      "Doing 71\n",
      "Doing 72\n",
      "Doing 73\n",
      "Doing 74\n",
      "Doing 75\n",
      "Doing 76\n",
      "Doing 77\n",
      "Doing 78\n",
      "Doing 79\n",
      "Doing 80\n",
      "Doing 81\n",
      "Doing 82\n",
      "Doing 83\n",
      "Doing 84\n",
      "Doing 85\n",
      "Doing 86\n",
      "Doing 87\n",
      "Doing 88\n",
      "Doing 89\n",
      "Doing 90\n",
      "Doing 91\n",
      "Doing 92\n",
      "Doing 93\n",
      "Doing 94\n",
      "Doing 95\n",
      "Doing 96\n",
      "Doing 97\n",
      "Doing 98\n",
      "Doing 99\n",
      "Doing 100\n",
      "Doing 101\n",
      "Doing 102\n",
      "Doing 103\n",
      "Doing 104\n",
      "Doing 105\n",
      "Doing 106\n",
      "Doing 107\n",
      "Doing 108\n",
      "Doing 109\n",
      "Doing 110\n",
      "Doing 111\n",
      "Doing 112\n",
      "Doing 113\n",
      "Doing 114\n",
      "Doing 115\n",
      "Doing 116\n",
      "Doing 117\n",
      "Doing 118\n",
      "Doing 119\n",
      "Doing 120\n",
      "Doing 121\n",
      "Doing 122\n",
      "Doing 123\n",
      "Doing 124\n",
      "Doing 125\n",
      "Doing 126\n",
      "Doing 127\n",
      "Doing 128\n",
      "Doing 129\n",
      "Doing 130\n",
      "Doing 131\n",
      "Doing 132\n",
      "Doing 133\n",
      "Doing 134\n",
      "Doing 135\n",
      "Doing 136\n",
      "Doing 137\n",
      "Doing 138\n",
      "Doing 139\n",
      "Doing 140\n",
      "Doing 141\n",
      "Doing 142\n",
      "Doing 143\n",
      "Doing 144\n",
      "Doing 145\n",
      "Doing 146\n",
      "Doing 147\n",
      "Doing 148\n",
      "Doing 149\n",
      "Doing 150\n",
      "Doing 151\n",
      "Doing 152\n",
      "Doing 153\n",
      "Doing 154\n",
      "Doing 155\n",
      "Doing 156\n",
      "Doing 157\n",
      "Doing 158\n",
      "Doing 159\n",
      "Doing 160\n",
      "Doing 161\n",
      "Doing 162\n",
      "Doing 163\n",
      "Doing 164\n",
      "Doing 165\n",
      "Doing 166\n",
      "Doing 167\n",
      "Doing 168\n",
      "Doing 169\n",
      "Doing 170\n",
      "Doing 171\n",
      "Doing 172\n",
      "Doing 173\n",
      "Doing 174\n",
      "Doing 175\n",
      "Doing 176\n",
      "Doing 177\n",
      "Doing 178\n",
      "Doing 179\n",
      "Doing 180\n",
      "Doing 181\n",
      "Doing 182\n",
      "Doing 183\n",
      "Doing 184\n",
      "Doing 185\n",
      "Doing 186\n",
      "Doing 187\n",
      "Doing 188\n",
      "Doing 189\n",
      "Doing 190\n",
      "Doing 191\n",
      "Doing 192\n",
      "Doing 193\n",
      "Doing 194\n",
      "Doing 195\n",
      "Doing 196\n",
      "Doing 197\n",
      "Doing 198\n",
      "Doing 199\n",
      "Doing 200\n",
      "Doing 201\n",
      "Doing 202\n",
      "Doing 203\n",
      "Doing 204\n",
      "Doing 205\n",
      "Doing 206\n",
      "Doing 207\n",
      "Doing 208\n",
      "Doing 209\n",
      "Doing 210\n",
      "Doing 211\n",
      "Doing 212\n",
      "Doing 213\n",
      "Doing 214\n",
      "Doing 215\n",
      "Doing 216\n",
      "Doing 217\n",
      "Doing 218\n",
      "Doing 219\n",
      "Doing 220\n",
      "Doing 221\n",
      "Doing 222\n",
      "Doing 223\n",
      "Doing 224\n",
      "Doing 225\n",
      "Doing 226\n",
      "Doing 227\n",
      "Doing 228\n",
      "Doing 229\n",
      "Doing 230\n",
      "Doing 231\n",
      "Doing 232\n",
      "Doing 233\n",
      "Doing 234\n",
      "Doing 235\n",
      "Doing 236\n",
      "Doing 237\n",
      "Doing 238\n",
      "Doing 239\n",
      "Doing 240\n",
      "Doing 241\n",
      "Doing 242\n",
      "Doing 243\n",
      "Doing 244\n",
      "Doing 245\n",
      "Doing 246\n",
      "Doing 247\n",
      "Doing 248\n",
      "Doing 249\n",
      "Doing 250\n",
      "Doing 251\n",
      "Doing 252\n",
      "Doing 253\n",
      "Doing 254\n",
      "Doing 255\n",
      "Doing 256\n",
      "Doing 257\n",
      "Doing 258\n",
      "Doing 259\n",
      "Doing 260\n",
      "Doing 261\n",
      "Doing 262\n",
      "Doing 263\n",
      "Doing 264\n",
      "Doing 265\n",
      "Doing 266\n",
      "Doing 267\n",
      "Doing 268\n",
      "Doing 269\n",
      "Doing 270\n",
      "Doing 271\n",
      "Doing 272\n",
      "Doing 273\n",
      "Doing 274\n",
      "Doing 275\n",
      "Doing 276\n",
      "Doing 277\n",
      "Doing 278\n",
      "Doing 279\n",
      "Doing 280\n",
      "Doing 281\n",
      "Doing 282\n",
      "Doing 283\n",
      "Doing 284\n",
      "Doing 285\n",
      "Doing 286\n",
      "Doing 287\n",
      "Doing 288\n",
      "Doing 289\n",
      "Doing 290\n",
      "Doing 291\n",
      "Doing 292\n",
      "Doing 293\n",
      "Doing 294\n",
      "Doing 295\n",
      "Doing 296\n",
      "Doing 297\n",
      "Doing 298\n",
      "Doing 299\n",
      "Doing 300\n"
     ]
    }
   ],
   "source": [
    "# Create a list of the items to scrape\n",
    "n_posts = range(1, 1000)\n",
    "df = pd.DataFrame(columns=['item','title', 'n_comments','corpus' ,'boost', 'sats', 'betha','commentors','external_links'])\n",
    "for number in n_posts:\n",
    "    # Initialize the dataframe\n",
    "\n",
    "    print(\"Doing\", number)\n",
    "    url_posts = f'https://stacker.news/items/{number}'\n",
    "    response = requests.get(url_posts)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    index = number-1\n",
    "#-------------------------------------------------------------------------------\n",
    "    # TITLE\n",
    "    try:\n",
    "        title = soup.find('a', class_='item_title__FH7AS text-reset me-2').get_text()\n",
    "    except:\n",
    "        continue\n",
    "    # add title to dataframe\n",
    "    df.at[index, 'title'] = title\n",
    "#-------------------------------------------------------------------------------\n",
    "    # ITEM\n",
    "    df.at[index, 'item'] = number\n",
    "#-------------------------------------------------------------------------------\n",
    "    # CORPUS\n",
    "    try:\n",
    "        corpus=soup.find('div', class_=\"item_fullItemContainer__ZAYtZ\").get_text()\n",
    "    except:\n",
    "        df.at[index, 'corpus'] = \"NaN\"\n",
    "    # add title to dataframe\n",
    "    df.at[index, 'corpus'] = corpus\n",
    "#-------------------------------------------------------------------------------\n",
    "    # EXTERNAL LINKS\n",
    "    try:\n",
    "        ex_link=soup.find('a', class_=\"item_link__4cWVs\").get_text()\n",
    "    except:\n",
    "        df.at[index, 'external_links'] = \"NaN\"\n",
    "    # add external links to dataframe\n",
    "    df.at[index, 'external_links'] = ex_link\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "    # BANNER\n",
    "    try:\n",
    "        banner = soup.find('div', class_='item_other__MjgP3')\n",
    "    except:\n",
    "        df.at[index, 'boost'] = \"NaN\"\n",
    "        df.at[index, 'sats'] = \"NaN\"\n",
    "        df.at[index, 'betha'] = \"NaN\"\n",
    "    # deal with banner\n",
    "    banner_data = []\n",
    "    for i in banner.find_all('span'):\n",
    "        banner_data.append(i.text)\n",
    "    for b in banner_data:\n",
    "        if \"boost\" in b:\n",
    "            df.at[index, 'boost'] = b\n",
    "        if \"sats\" in b:\n",
    "            df.at[index, 'sats'] = b\n",
    "        if \"@\" in b:\n",
    "            df.at[index, 'betha'] = b\n",
    "#-------------------------------------------------------------------------------\n",
    "    # N_COMMENTS\n",
    "    try:\n",
    "        n_comments = soup.find('a', class_='text-reset position-relative').get_text()\n",
    "    except:        \n",
    "        df.at[index, 'n_comments'] = \"NaN\"\n",
    "    # add n_comments to dataframe\n",
    "    df.at[index, 'n_comments'] = n_comments\n",
    "#-------------------------------------------------------------------------------\n",
    "    # COMMENTORS\n",
    "    \n",
    "    a_elements = soup.find_all('a')\n",
    "    \n",
    "    df.at[index, 'commentors'] = \"NaN\"\n",
    "    at_elements = []\n",
    "\n",
    "    for el in a_elements:\n",
    "        links = el.get_text() \n",
    "        if links.startswith('@'):\n",
    "            at_elements.append(el)\n",
    "    \n",
    "    commentors_list=[]\n",
    "    for ind in range(0,len(at_elements)):\n",
    "        commentors_list.append(at_elements[ind][\"href\"])\n",
    "    # add commentors to dataframe\n",
    "    df.at[index, 'commentors'] = commentors_list\n",
    "#-------------------------------------------------------------------------------\n",
    "# Fixing the dataframe\n",
    "df['author'] = df['betha'].str.extract(r'@(\\w+)')\n",
    "df['data'] = df['betha'].str.extract(r'(\\d+\\s\\w+\\s\\d+)')\n",
    "df.drop('betha', axis=1, inplace=True)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
